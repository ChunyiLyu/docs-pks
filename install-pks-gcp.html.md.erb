---
title: Installing PKS on GCP
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to install and configure Pivotal Container Service (PKS) on GCP.

##<a id='prerequisites'></a>Prerequisites

Before performing the procedures in this topic, you must have deployed and configured Ops Manager.
For more information, see [GCP Prerequisites and Resource Requirements](gcp-requirements.html).

If you use an instance of Ops Manager that you configured previously to install other runtimes, confirm the following settings before you install PKS:

1. Navigate to Ops Manager.
1. Open the **Director Config** pane.
1. Select the **Enable Post Deploy Scripts** checkbox.
1. Clear the **Disable BOSH DNS server for troubleshooting purposes** checkbox.
1. Click the **Installation Dashboard** link to return to the Installation Dashboard.
1. Click **Apply Changes**.

##<a id='install'></a> Step 1: Install PKS

To install PKS, do the following:

1. Download the product file from [Pivotal Network](https://network.pivotal.io).
1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.
1. Click **Import a Product** to upload the product file.
1. Under **Pivotal Container Service** in the left column, click the plus sign to add this product to your staging area.

##<a id='configure'></a> Step 2: Configure PKS

Click the orange **Pivotal Container Service** tile to start the configuration process.

![Pivotal Container Service tile on the Ops Manager installation dashboard](images/pks-tile-orange.png)

###<a id='azs-networks'></a> Assign AZs and Networks

Perform the following steps:

1. Click **Assign AZs and Networks**.

1. Select the availability zone (AZ) where you want to deploy the PKS API VM as a singleton job.
  <p class="note"><strong>Note</strong>: You must select an additional AZ for balancing other jobs before clicking <strong>Save</strong>, but this selection has no effect in the current version of PKS.</p>

    ![Assign AZs and Networks pane in Ops Manager](images/azs-networks.png)

1. Under **Network**, select the infrastructure subnet you created for the PKS API VM.
1. Under **Service Network**, select the services subnet you created for Kubernetes cluster VMs.
1. Click **Save**.

###<a id='pks-api'></a> PKS API

Perform the following steps:

1. Click **PKS API**.
1. Under **Certificate to secure the PKS API**, provide your own certificate and private key pair.
  ![PKS API pane configuration](images/pks-api.png)
The certificate you enter here should cover the domain that routes to the PKS API VM with TLS termination on the ingress.<br><br>
    (Optional) If you do not have a certificate and private key pair, you can have Ops Manager generate one for you. Perform the following steps:
  1. Select the **Generate RSA Certificate** link.
  1. Enter the wildcard domain for your API hostname.
    For example, if your PKS API domain is `api.pks.example.com`, then enter `*.pks.example.com`.
  1. Click **Generate**.
1. Under **API Hostname (FQDN)**, enter a fully qualified domain name (FQDN) to access the PKS API.
For example, `api.pks.example.com`.
1. Click **Save**.

###<a id='usage'></a> Usage Data

VMware's Customer Experience Improvement Program (CEIP) and the Pivotal Telemetry Program
(Telemetry) provides VMware and Pivotal with information that enables the companies to improve their
products and services, fix problems, and advise you on how best to deploy and use our products.
As part of the CEIP and Telemetry, VMware and Pivotal collect technical information about your
organization's use of the Pivotal Container Service ("PKS") on a regular basis.
Since PKS is jointly developed and sold by VMware and Pivotal, we will share this information with one
another.
Information collected under CEIP or Telemetry does not personally identify any individual.

Regardless of your selection in the **Usage Data** pane, a small amount of data is sent from Cloud Foundry Container Runtime (CFCR) to the PKS tile. However, that data is not shared externally. 

To configure the **Usage Data** pane:

1. Select the **Usage Data** side-tab.
1. Read the Usage Data description.
1. Make your selection.
  1. To join the program, select **Yes, I want to join the CEIP and Telemetry Program for PKS**.
  1. To decline joining the program, select **No, I do not want to join the CEIP and Telemetry Program for PKS**.
1. Click **Save**.

<p class="note"><strong>Note:</strong> If you join the CEIP and Telemetry Program for PKS, open your firewall to allow outgoing access to <code>https://vcsa.vmware.com/ph-prd</code> on port <code>443</code>.
</p>

###<a id='plans'></a> Plans

To activate a plan, perform the following steps:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
  <p class="note"><strong>Note</strong>: A plan defines a set of resource types used for deploying clusters. You can configure up to three plans. You must configure <strong>Plan 1</strong>.</p>
1. Select **Active** to activate the plan and make it available to developers deploying clusters.
  ![Plan pane configuration](images/plan1.png)
1. Under **Name**, provide a unique name for the plan.
1. Under **Description**, edit the description as needed.
The plan description appears in the Services Marketplace, which developers can access by using PKS CLI.
1. Under **Master/ETCD Node Instances**, select the default number of Kubernetes master/etcd nodes to provision for each cluster. You can enter either <code>1</code> or <code>3</code>. For increased master node availability, set this value to <code>3</code>.
  <p class="note warning"><strong>WARNING</strong>: To change the number of master/etcd nodes for a plan, you must ensure that no existing clusters use the plan. PKS does not support changing the number of master/etcd nodes for plans with existing clusters.
  </p>
<%= partial 'beta-component' %>
1. Under **Master/ETCD VM Type**, select the type of VM to use for Kubernetes master/etcd nodes. For more information, see the [Master Node VM Size](vm-sizing.html#master-sizing) section of _VM Sizing for PKS Clusters_.
1. Under **Master Persistent Disk Type**, select the size of the persistent disk for the Kubernetes master node VM.
1. Under **Master/ETCD Availability Zones**, select one or more AZs for the Kubernetes clusters deployed by PKS. If you select more than one AZ, PKS deploys the master VM in the first AZ and the worker VMs across the remaining AZs.
1. Under **Worker Node Instances**, select the default number of Kubernetes worker nodes to provision for each cluster. For high availability, create clusters with a minimum of three worker nodes, or two per AZ if you intend to use persistent volumes. For example, if you deploy across three AZs, you should have six worker nodes. For more information about persistent volumes, see [Persistent Volumes](maintain-uptime.html#persistent-volumes) in _Maintain Workload Uptime_.  Provisioning a minimum of three worker nodes, or two nodes per AZ is also recommended for stateless workloads.

    <img src="images/plan2.png" alt="Plan pane configuration, part two" width="375">
1. Under **Worker VM Type**, select the type of VM to use for Kubernetes worker node VMs. For more information, see the [Worker Node VM Number and Size](vm-sizing.html#worker-sizing) section of _VM Sizing for PKS Clusters_.
1. Under **Worker Persistent Disk Type**, select the size of the persistent disk for the Kubernetes worker node VMs.
1. Under **Worker Availability Zones**, select one or more AZs for the Kubernetes worker nodes. PKS deploys worker nodes equally across the AZs you select.
1. Under **Errand VM Type**, select the size of the VM that contains the errand. The smallest instance possible is sufficient, as the only errand running on this VM is the one that applies the **Default Cluster App** YAML configuration.
1. (Optional) Under **(Optional) Add-ons - Use with caution**, enter additional YAML configuration to add custom workloads to each cluster in this plan. You can specify multiple files using `---` as a separator. For more information, see [Add Custom Workloads](custom-workloads.html).
  ![Plan pane configuration](images/plan3.png)
1. (Optional) To allow users to create pods with privileged containers, select the **Enable Privileged Containers - Use with caution** option. For more information, see [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers) in the Kubernetes documentation.
1. (Optional) To disable the admission controller, select the **Disable DenyEscalatingExec** checkbox. If you select this option, clusters in this plan can create security vulnerabilities that may impact other tiles. Use this feature with caution.
1. Click **Save**.

To deactivate a plan, perform the following steps:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
1. Select **Plan Inactive**.
1. Click **Save**.

###<a id='cloud-provider'></a> Kubernetes Cloud Provider

To configure your Kubernetes cloud provider settings, follow the procedures below:

1. Click **Kubernetes Cloud Provider**.

1. Under **Choose your IaaS**, select **GCP**.

1. Ensure the values in the following procedure match those in the **Google Config** section of the **Ops Manager** tile as follows:

    <img src="images/cloud-gcp.png" alt="GCP pane configuration" width="325">

    1. Enter your **GCP Project Id**, which is the name of the deployment in your Ops Manager environment.

    1. Enter your **VPC Network**, which is the VPC network name for your Ops Manager environment.

    1. Enter your **GCP Master Service Account ID**. This is the email address associated with the master node service account. For information about configuring this account, see [Create the Master Node Service Account](gcp-prepare-env.html#create-master).

    1. Enter your **GCP Worker Service Account ID**. This is the email address associated with the worker node service account. For information about configuring this account, see [Create the Worker Node Service Account](gcp-prepare-env.html#create-worker).

1. Click **Save**.

###<a id='syslog'></a> (Optional) Logging

You can designate an external syslog endpoint for PKS component and cluster log messages.

To specify the destination for PKS log messages, do the following:

1. Click **Logging**.
1. To enable syslog forwarding, select **Yes**.

    ![Enable syslog forwarding](images/logging.png)

1. Under **Address**, enter the destination syslog endpoint.
1. Under **Port**, enter the destination syslog port.
1. Select a transport protocol for log forwarding.
1. (Optional) Pivotal strongly recommends that you enable TLS encryption when forwarding logs as they may contain sensitive information. For example, these logs may contain cloud provider credentials. To enable TLS, perform the following steps:
  1. Under **Permitter Peer**, provide the accepted fingerprint (SHA1) or name of remote peer. For example, `*.YOUR-LOGGING-SYSTEM.com`.
  1. Under **TLS Certificate**, provide a TLS certificate for the destination syslog endpoint.
  <p class="note"><strong>Note</strong>: You do not need to provide a new certificate if the TLS certificate for the destination syslog endpoint is signed by a Certificate Authority (CA) in your BOSH certificate store.
  </p>

1. Click **Save**.

###<a id='networking'></a> Networking

To configure networking, do the following:

1. Click **Networking**.
1. Under **Container Networking Interface**, select **Flannel**.
  <img src="images/networking-flannel.png" alt="Networking pane configuration" width="425">

1. (Optional) If you do not use a NAT instance, select **Allow outbound internet access from Kubernetes cluster vms (IaaS-dependent)**. Enabling this functionality assigns external IP addresses to VMs in clusters.

1. Click **Save**.

###<a id='uaa'></a> UAA

To configure the UAA server, do the following:

1. Click **UAA**.
1. Under **PKS CLI Access Token Lifetime**, enter a time in seconds for the PKS CLI access token lifetime.<br>
  <img src="images/uaa.png" alt="UAA pane configuration" width="325">
1. Under **PKS CLI Refresh Token Lifetime**, enter a time in seconds for the PKS CLI refresh token lifetime.
1. Select one of the following options:
  * To use an internal user account store for UAA, select **Internal UAA**. Click **Save** and continue to [(Optional) Monitoring](#monitoring).
  * To use an external user account store for UAA, select **LDAP Server** and continue to [Configure LDAP as an Identity Provider](#configure-ldap).

#### <a id="configure-ldap"></a>Configure LDAP as an Identity Provider

To integrate UAA with one or more LDAP servers, configure PKS with your LDAP endpoint information as follows:

1. Under **UAA**, select **LDAP Server**.<br>
  ![LDAP Server configuration pane](images/ldap1.png)

1. For **Server URL**, enter the URLs that point to your LDAP server.
If you have multiple LDAP servers, separate their URLs with spaces.
Each URL must include one of the following protocols:
    * `ldap://`: Use this protocol if your LDAP server uses an unencrypted connection.
    * `ldaps://`: Use this protocol if your LDAP server uses SSL for an encrypted connection.
To support an encrypted connection, the LDAP server must hold a trusted certificate or you must import a trusted certificate to the JVM truststore.

1. For **LDAP Credentials**, enter the LDAP Distinguished Name (DN) and password for binding to the LDAP server.
For example, `cn=administrator,ou=Users,dc=example,dc=com`.
If the bind user belongs to a different search base, you must use the full DN.
    <p class="note"><strong>Note</strong>: We recommend that you provide LDAP credentials that grant read-only permissions on the LDAP search base and the LDAP group search base.</p>

1. For **User Search Base**, enter the location in the LDAP directory tree where LDAP user search begins.
The LDAP search base typically matches your domain name.
<br><br>
For example, a domain named `cloud.example.com` may use `ou=Users,dc=example,dc=com` as its LDAP user search base.

1. For **User Search Filter**, enter a string to use for LDAP user search criteria.
The search criteria allows LDAP to perform more effective and efficient searches.
For example, the standard LDAP search filter `cn=Smith` returns all objects with a common name equal to `Smith`.
<br><br>
In the LDAP search filter string that you use to configure PKS, use `{0}` instead of the username.
For example, use `cn={0}` to return all LDAP objects with the same common name as the username.
<br><br>
In addition to `cn`, other common attributes are `mail`, `uid` and, in the case of Active Directory, `sAMAccountName`.
    <p class="note"><strong>Note</strong>: For information about testing and troubleshooting your LDAP search filters, see <a href="https://community.pivotal.io/s/article/Configuring-LDAP-Integration-with-Pivotal-Cloud-Foundry">Configuring LDAP Integration with Pivotal Cloud Foundry</a>.</p>

1. For **Group Search Base**, enter the location in the LDAP directory tree where the LDAP group search begins.
<br><br>
For example, a domain named `cloud.example.com` may use `ou=Groups,dc=example,dc=com` as its LDAP group search base.
<br><br>
Follow the instructions in the [Grant Cluster Access to an External LDAP Group](manage-users.html#external-group) section 
of _Creating and Managing Users with the UAA CLI (UAAC)_ to map the groups under this search base to roles in PKS.

1. For **Group Search Filter**, enter a string that defines LDAP group search criteria. The standard value is `member={0}`.

1. For **Server SSL Cert**, paste in the root certificate from your CA certificate or your self-signed certificate.<br>
  ![LDAP Server configuration pane](images/ldap2.png)

1. For **Server SSL Cert AltName**, do one of the following:
  * If you are using `ldaps://` with a self-signed certificate, enter a Subject Alternative Name (SAN) for your certificate.
  * If you are not using `ldaps://` with a self-signed certificate, leave this field blank.

1. For **First Name Attribute**, enter the attribute name in your LDAP directory that contains user first names.
For example, `cn`.

1. For **Last Name Attribute**, enter the attribute name in your LDAP directory that contains user last names. 
For example, `sn`.

1. For **Email Attribute**, enter the attribute name in your LDAP directory that contains user email addresses.
For example, `mail`.

1. For **Email Domain(s)**, enter a comma-separated list of the email domains for external users who can receive invitations to Apps Manager.

1. For **LDAP Referrals**, choose how UAA handles LDAP server referrals to other user stores.
UAA can follow the external referrals, ignore them without returning errors, or generate an error for each external referral and abort the authentication.

1. Click **Save**.

#### <a id="configure-oidc"></a>(Optional) Configure OpenID Connect

You can use OpenID Connect (OIDC) to instruct Kubernetes to verify end-user identities based on authentication performed by an authorization server, such as UAA.

To configure PKS to use OIDC, select **Enable UAA as OIDC provider**.

  ![OIDC configuration checkbox](images/oidc1.png)

For more information about configuring OIDC, see the table below:

<table class="nice">
  <tr>
    <th>Option</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>OIDC disabled</td>
    <td>If you do not enable OIDC, Kubernetes authenticates users against its internal user management system.</td>
  </tr>
  <tr>
    <td>OIDC enabled</td>
    <td>If you enable OIDC, Kubernetes uses the authentication mechanism that you selected in <a href="#uaa">UAA</a>:
      <ul>
        <li>If you selected <strong>Internal UAA</strong>, Kubernetes authenticates users against the internal UAA authentication mechanism.</li>
        <li>If you selected <strong>LDAP Server</strong>, Kubernetes authenticates users against the LDAP server.</li>
      </ul>
    </td>
  </tr>
</table>

<p class="note"><strong>Note</strong>: When you enable OIDC, existing PKS-provisioned Kubernetes clusters are upgraded to use OIDC. This invalidates your kubeconfig files. You must regenerate the files for all clusters.</p>

###<a id='monitoring'></a> (Optional) Monitoring

You can monitor Kubernetes clusters and pods metrics externally using the integration with [Wavefront by VMware](https://www.wavefront.com/).
<p class="note"><strong>Note</strong>: Before you configure the Wavefront integration, you must have an active Wavefront account and access to a Wavefront instance. You provide your Wavefront access token during configuration. For instructions and additional information, see the [Wavefront documentation](https://docs.wavefront.com/).</p>

By default, monitoring is disabled. To enable and configure Wavefront monitoring, do the following:

1. Under **Wavefront Integration**, select **Yes**.<br>
  ![Monitoring pane configuration](images/monitoring.png)
1. Under **Wavefront URL**, enter the URL of your Wavefront subscription. For example, `https://try.wavefront.com/api`.
1. Under **Wavefront Access Token**, enter the API token for your Wavefront subscription.
1. To configure Wavefront to send alerts by email, enter email addresses or Wavefront Target IDs separated by commas under **Wavefront Alert Recipient**. For example: `user@example.com,Wavefront_TargetID`. You must enable errands to create alerts.
<p class="note"><strong>Note</strong>: You must enable errands to create alerts. In the <strong>Errands</strong> tab, enable the <strong>Create pre-defined Wavefront alerts errand</strong> and <strong>Delete pre-defined Wavefront alerts errand</strong>.</p>
1. Click **Save**. Your settings apply to any clusters created after you have saved these configuration settings and clicked **Apply Changes**.
<p class="note"><strong>Note</strong>: The PKS tile does not validate your Wavefront configuration settings. To verify your setup, look for cluster and pod metrics in Wavefront.</p>

###<a id='errands'></a> Errands

Errands are scripts that run at designated points during an installation.

To configure when post-deploy and pre-delete errands for PKS are run, make a selection in the dropdown next to the errand. For a typical PKS deployment, we recommend that you leave the default settings.

  ![Errand configuration pane](images/errands.png)

For more information about errands and their configuration state, see [Managing Errands in Ops Manager](https://docs.pivotal.io/pivotalcf/customizing/managing_errands.html).

<p class="note warning"><strong>WARNING</strong>: Because PKS uses floating stemcells, updating the PKS tile with a new stemcell triggers the rolling of every VM in each cluster. Also, updating other product tiles in your deployment with a new stemcell causes the PKS tile to roll VMs. This rolling is enabled by the <strong>Upgrade all clusters errand</strong>. We recommend that you keep this errand turned on because automatic rolling of VMs ensures that all deployed cluster VMs are patched. However, automatic rolling can cause downtime in your deployment.
<br><br>
If you upgrade PKS from 1.0.x to 1.1, you must enable the <strong>Upgrade All Cluster</strong> errand. This ensures existing clusters can perform resize or delete actions after the upgrade.</p>

###<a id='resource-config'></a> Resource Config

To modify the resource usage of PKS and specify your PKS API load balancer, follow the steps below:

1. Select **Resource Config**.

1. (Optional) Edit resources used by the **Pivotal Container Service** job.

    ![Resource pane configuration](images/resources.png)

1. In the **Load Balancers** column, enter a name for your PKS API load balancer that begins with `tcp:`.
For example, `tcp:pks-api`, where `pks-api` is the name that you configured in the [Create a Load Balancer](gcp-api-load-balancer.html#create-lb) step _Configuring a GCP Load Balancer for the PKS API_.

<p class="note"><strong>Note</strong>: If you experience timeouts or slowness when interacting with the PKS API, select a <strong>VM Type</strong> with greater CPU and memory resources for the <strong>Pivotal Container Service</strong> job.</p>

## <a id='apply-changes'></a> Step 3: Apply Changes

After configuring the tile, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy the tile.

##<a id='next-steps'></a> Next Steps

After installing PKS on GCP, you may want to do one or more of the following:

* [Share PKS API Endpoint](#endpoint)
* [Install the PKS and Kubernetes CLIs](#clis)
* [Configure Authentication for PKS](#auth)

### <a id='clis'></a> Install the PKS and Kubernetes CLIs

The PKS and Kubernetes CLIs help you interact with your PKS-provisioned Kubernetes clusters and Kubernetes workloads. To install the CLIs, follow the instructions below:

* [Installing the PKS CLI](installing-pks-cli.html)

* [Installing the Kubernetes CLI](installing-kubectl-cli.html)

### <a id='endpoint'></a> Share PKS API Endpoint

You must share the PKS API endpoint to allow your organization to use the API to create, update, and delete clusters.
See [Create a Cluster](create-cluster.html) for more information.

To retrieve the PKS API endpoint, do the following:

1. Navigate to the Ops Manager Installation Dashboard.
1. Click the Pivotal Container Service tile.
1. Click the **Status** tab and locate the **Pivotal Container Service** job. The IP address of the Pivotal Container Service job is the PKS API endpoint.

### <a id='auth'></a> Configure Authentication for PKS

To configure authentication for PKS, do the following:

1. Configure authentication for PKS using either User Account and Authentication (UAA) or enterprise single sign-on (SSO). To create and manage users using UAA, see [Manage Users in UAA](manage-users.html).

1. After configuring authentication, follow the procedures in [Configure PKS API Access](configure-api.html) to enable operators to create and manage clusters.
