---
title: Installing PKS on vSphere
owner: PKS
iaas: vSphere
---

<strong><%= modified_date %></strong>

This topic describes how to install and configure Pivotal Container Service (PKS) on vSphere.

##<a id='prerequisites'></a>Prerequisites

Before performing the procedures in this topic, you must have deployed and configured Ops Manager.
For more information, see [vSphere Prerequisites and Resource Requirements](vsphere-requirements.html).

If you use an instance of Ops Manager that you configured previously to install other runtimes, confirm the following settings before you install PKS:

1. Navigate to Ops Manager.
1. Open the **Director Config** pane.
1. Select the **Enable Post Deploy Scripts** checkbox.
1. Clear the **Disable BOSH DNS server for troubleshooting purposes** checkbox.
1. Click the **Installation Dashboard** link to return to the Installation Dashboard.
1. Click **Apply Changes**.

##<a id='install'></a> Step 1: Install PKS

To install PKS, do the following:

1. Download the product file from [Pivotal Network](https://network.pivotal.io).
1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.
1. Click **Import a Product** to upload the product file.
1. Under **Pivotal Container Service** in the left column, click the plus sign to add this product to your staging area.

##<a id='configure'></a> Step 2: Configure PKS

Click the orange **Pivotal Container Service** tile to start the configuration process.

![Pivotal Container Service tile on the Ops Manager installation dashboard](images/pks-tile-orange.png)

###<a id='azs-networks'></a> Assign AZs and Networks

Perform the following steps:

1. Click **Assign AZs and Networks**.

1. Select the availability zone (AZ) where you want to deploy the PKS API VM as a singleton job.
  <p class="note"><strong>Note</strong>: You must select an additional AZ for balancing other jobs before clicking <strong>Save</strong>, but this selection has no effect in the current version of PKS.</p>

    ![Assign AZs and Networks pane in Ops Manager](images/azs-networks.png)

1. Under **Network**, select the infrastructure subnet you created for the PKS API VM.
1. Under **Service Network**, select the services subnet you created for Kubernetes cluster VMs.
1. Click **Save**.

###<a id='pks-api'></a> PKS API

Perform the following steps:

1. Click **PKS API**.
1. Under **Certificate to secure the PKS API**, provide your own certificate and private key pair.
  ![PKS API pane configuration](images/pks-api.png)
The certificate you enter here should cover the domain that routes to the PKS API VM with TLS termination on the ingress.<br><br>
    (Optional) If you do not have a certificate and private key pair, you can have Ops Manager generate one for you. Perform the following steps:
  1. Select the **Generate RSA Certificate** link.
  1. Enter the wildcard domain for your API hostname.
    For example, if your PKS API domain is `api.pks.example.com`, then enter `*.pks.example.com`.
  1. Click **Generate**.
1. Under **API Hostname (FQDN)**, enter a fully qualified domain name (FQDN) to access the PKS API.
For example, `api.pks.example.com`.
1. Click **Save**.

###<a id='plans'></a> Plans

To activate a plan, perform the following steps:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
  <p class="note"><strong>Note</strong>: A plan defines a set of resource types used for deploying clusters. You can configure up to three plans. You must configure <strong>Plan 1</strong>.</p>
1. Select **Active** to activate the plan and make it available to developers deploying clusters.
  ![Plan pane configuration](images/plan1.png)
1. Under **Name**, provide a unique name for the plan.
1. Under **Description**, edit the description as needed.
The plan description appears in the Services Marketplace, which developers can access by using PKS CLI.
1. Under **Master/ETCD Node Instances**, select the default number of Kubernetes master/etcd nodes to provision for each cluster. You can enter either <code>1</code> or <code>3</code>. For increased master node availability, set this value to <code>3</code>.
  <p class="note warning"><strong>WARNING</strong>: To change the number of master/etcd nodes for a plan, you must ensure that no existing clusters use the plan. PKS does not support changing the number of master/etcd nodes for plans with existing clusters.
  </p>
<%= partial 'beta-component' %>
1. Under **Master/ETCD VM Type**, select the type of VM to use for Kubernetes master/etcd nodes. For more information, see the [Master Node VM Size](vm-sizing.html#master-sizing) section of _VM Sizing for PKS Clusters_.
1. Under **Master Persistent Disk Type**, select the size of the persistent disk for the Kubernetes master node VM.
1. Under **Master/ETCD Availability Zones**, select one or more AZs for the Kubernetes clusters deployed by PKS. If you select more than one AZ, PKS deploys the master VM in the first AZ and the worker VMs across the remaining AZs.
1. Under **Worker Node Instances**, select the default number of Kubernetes worker nodes to provision for each cluster. For high availability, create clusters with a minimum of three worker nodes, or two per AZ if you intend to use persistent volumes. For example, if you deploy across three AZs, you should have six worker nodes. For more information about persistent volumes, see [Persistent Volumes](maintain-uptime.html#persistent-volumes) in _Maintain Workload Uptime_.  Provisioning a minimum of three worker nodes, or two nodes per AZ is also recommended for stateless workloads.

    <img src="images/plan2.png" alt="Plan pane configuration, part two" width="375">
1. Under **Worker VM Type**, select the type of VM to use for Kubernetes worker node VMs. For more information, see the [Worker Node VM Number and Size](vm-sizing.html#worker-sizing) section of _VM Sizing for PKS Clusters_.
1. Under **Worker Persistent Disk Type**, select the size of the persistent disk for the Kubernetes worker node VMs.
1. Under **Worker Availability Zones**, select one or more AZs for the Kubernetes worker nodes. PKS deploys worker nodes equally across the AZs you select.
1. Under **Errand VM Type**, select the size of the VM that contains the errand. The smallest instance possible is sufficient, as the only errand running on this VM is the one that applies the **Default Cluster App** YAML configuration.
1. (Optional) Under **(Optional) Add-ons - Use with caution**, enter additional YAML configuration to add custom workloads to each cluster in this plan. You can specify multiple files using `---` as a separator. For more information, see [Add Custom Workloads](custom-workloads.html).
  ![Plan pane configuration](images/plan3.png)
1. (Optional) To allow users to create pods with privileged containers, select the **Enable Privileged Containers - Use with caution** option. For more information, see [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers) in the Kubernetes documentation.
1. (Optional) To disable the admission controller, select the **Disable DenyEscalatingExec** checkbox. If you select this option, clusters in this plan can create security vulnerabilities that may impact other tiles. Use this feature with caution.
1. Click **Save**.

To deactivate a plan, perform the following steps:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
1. Select **Plan Inactive**.
1. Click **Save**.

###<a id='cloud-provider'></a> Kubernetes Cloud Provider

In the procedure below, you use credentials for vCenter master VMs.
You must have provisioned the service account with the correct permissions.
For more information, see [Create the Master Node Service Account](vsphere-prepare-env.html#create-master).

To configure your Kubernetes cloud provider settings, follow the procedure below:

1. Click **Kubernetes Cloud Provider**.
1. Under **Choose your IaaS**, select **vSphere**.
1. Ensure the values in the following procedure match those in the **vCenter Config** section of the **Ops Manager** tile.

    <img src="images/cloud-vsphere.png" alt="vSphere pane configuration" width="325">

    1. Enter your **vCenter Master Credentials**. Enter the username using the format `user@CF-EXAMPLE.com`. For more information about the master node service account, see [Preparing to Deploy PKS on vSphere](vsphere-prepare-env.html).
    1. Enter your **vCenter Host**. For example, `vcenter.CF-EXAMPLE.com`.
    1. Enter your **Datacenter Name**. For example, `CF-EXAMPLE-dc`.
    1. Enter your **Datastore Name**. For example, `CF-EXAMPLE-ds`.
    1. Enter the **Stored VM Folder** so that the persistent stores know where to find the VMs. To retrieve the name of the folder, navigate to your BOSH Director tile, click **vCenter Config**, and locate the value for **VM Folder**. The default folder name is `pcf_vms`.
  <p class="note"><strong>Note</strong>: We recommend using a shared datastore for multi-AZ and multi-cluster environments.</p>
1. Click **Save**.

###<a id='syslog'></a> (Optional) Logging

<%= partial 'logging' %>

###<a id='networking'></a> Networking

To configure networking, do the following:

1. Click **Networking**.
1. Under **Container Networking Interface**, select **Flannel**.
  <img src="images/networking-flannel.png" alt="Networking pane configuration" width="425">
1. (Optional) Configure a global proxy for all outgoing HTTP and HTTPS traffic from your Kubernetes clusters. 
<br><br>
Production environments can deny direct access to public Internet services and between internal services by placing an HTTP or HTTPS proxy in the network path between Kubernetes nodes and those services.
<br><br>
If your environment includes HTTP or HTTPS proxies, configuring PKS to use these proxies allows PKS-deployed Kubernetes nodes to access public Internet services and other internal services. Follow the steps below to configure a global proxy for all outgoing HTTP/HTTPS traffic from your Kubernetes clusters:
  1. Under **HTTP/HTTPS proxy**, select **Enabled**.
      <img src="images/networking-https-proxy.png" alt="Networking pane configuration" width="325">
  1. Under **HTTP Proxy URL**, enter the URL of your HTTP/HTTPS proxy endpoint. For example, `http://myproxy.com:1234`.
  1. (Optional) If your proxy uses basic authentication, enter the username and password under **HTTP Proxy Credentials**.
  1. Under **No Proxy**, enter the service network CIDR where your PKS cluster is deployed. List any additional IP addresses that should bypass the proxy.
    <p class="note"><strong>Note</strong>: By default, the <code>.internal</code>, <code>10.100.0.0/8</code>, and <code>10.200.0.0/8</code> IP address ranges are not proxied. This allows internal PKS communication.</p>
1. Under **Allow outbound internet access from Kubernetes cluster vms (IaaS-dependent)**, ignore the **Enable outbound internet access** checkbox.
1. Click **Save**.

###<a id='uaa'></a> UAA

<%= partial 'uaa' %>

###<a id='monitoring'></a> (Optional) Monitoring

<%= partial 'monitoring' %>

###<a id='usage'></a> Usage Data

<%= partial 'usage-data' %>

###<a id='errands'></a> Errands

Errands are scripts that run at designated points during an installation.

To configure when post-deploy and pre-delete errands for PKS are run, make a selection in the dropdown next to the errand. For a typical PKS deployment, we recommend that you leave the default settings.

  ![Errand configuration pane](images/errands.png)

For more information about errands and their configuration state, see [Managing Errands in Ops Manager](https://docs.pivotal.io/pivotalcf/customizing/managing_errands.html).

<p class="note warning"><strong>WARNING</strong>: Because PKS uses floating stemcells, updating the PKS tile with a new stemcell triggers the rolling of every VM in each cluster. Also, updating other product tiles in your deployment with a new stemcell causes the PKS tile to roll VMs. This rolling is enabled by the <strong>Upgrade all clusters errand</strong>. We recommend that you keep this errand turned on because automatic rolling of VMs ensures that all deployed cluster VMs are patched. However, automatic rolling can cause downtime in your deployment.
<br><br>
If you upgrade PKS from 1.0.x to 1.1, you must enable the <strong>Upgrade All Cluster</strong> errand. This ensures existing clusters can perform resize or delete actions after the upgrade.</p>

###<a id='resource-config'></a> Resource Config

To modify the resource usage of PKS, click **Resource Config** and edit the **Pivotal Container Service** job.

![Resource pane configuration](images/resources.png)

<p class="note"><strong>Note</strong>: If you experience timeouts or slowness when interacting with the PKS API, select a <strong>VM Type</strong> with greater CPU and memory resources for the <strong>Pivotal Container Service</strong> job.</p>

## <a id='apply-changes'></a> Step 3: Apply Changes

After configuring the tile, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy the tile.

##<a id='retrieve-pks-api'></a> Step 4: Retrieve PKS API Endpoint

You must share the PKS API endpoint to allow your organization to use the API to create, update, and delete clusters.
See [Create a Cluster](create-cluster.html) for more information.

To retrieve the PKS API endpoint, do the following:

1. Navigate to the Ops Manager Installation Dashboard.
1. Click the Pivotal Container Service tile.
1. Click the **Status** tab and locate the **Pivotal Container Service** job. The IP address of the Pivotal Container Service job is the PKS API endpoint.

## <a id='loadbalancer-pks-api'></a> Step 5: Configure External Load Balancer

After you install the PKS tile, configure an external load balancer to access the PKS API from outside the network.
You can use any external load balancer.

Your external load balancer forwards traffic to the PKS API endpoint on ports 8443 and 9021.
Configure the external load balancer to resolve to the domain name you set in the [PKS API](#pks-api) section of the tile configuration.

Configure your load balancer with the following information:

- IP address from [Retrieve PKS API Endpoint](#retrieve-pks-api)
- Ports 8443 and 9021
- HTTPS or TCP protocol

##<a id='next-steps'></a> Next Steps

After installing PKS on vSphere, you may want to do one or more of the following:

* [Install the PKS and Kubernetes CLIs](#clis)
* [Configure Authentication for PKS](#auth)
* [Integrate VMware Harbor with PKS](#harbor)

### <a name='clis'></a> Install the PKS and Kubernetes CLIs

The PKS and Kubernetes CLIs help you interact with your PKS-provisioned Kubernetes clusters and Kubernetes workloads. To install the CLIs, follow the instructions below:

* [Installing the PKS CLI](installing-pks-cli.html)

* [Installing the Kubernetes CLI](installing-kubectl-cli.html)

### <a name='auth'></a> Configure Authentication for PKS

To configure authentication for PKS, do the following:

1. Configure authentication for PKS using either User Account and Authentication (UAA) or enterprise single sign-on (SSO). To create and manage users using UAA, see [Manage Users in UAA](manage-users.html).

1. After configuring authentication, follow the procedures in [Configure PKS API Access](configure-api.html) to enable operators to create and manage clusters.

### <a name='harbor'></a> Integrate VMware Harbor with PKS

To integrate VMware Harbor Registry with PKS to store and manage container images, see [Integrating VMware Harbor Registry with PKS](https://docs.pivotal.io/partners/vmware-harbor/integrating-pks.html).
