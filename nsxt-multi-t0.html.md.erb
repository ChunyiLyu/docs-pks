---
title: Multi-T0 for Customer/Tenant Isolation
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to define network profiles for Kubernetes clusters provisioned with PKS on vSphere with NSX-T.

## <a id='about'></a> About Multi-T0 Router for Customer/Tenant Isolation

Multi-T0 lets you manage and secure Kubernetes cluster deployments on isolated networks for multiple customers or tenants. This section provides instructions for setting up a multi-T0 environment for PKS. After completing these steps you be able to deploy multiple customer/tenant clusters in their own T0 network. See Advanced Configuration for security enhancements to the base configuration.

## <a id="base-config"></a> Base Configuration

### Step 1. Verify PKS deployment

PKS 1.2.x is deployed with NSX-T 2.3.x using NAT-mode or routes with at least one edge cluster and two edge nodes.

### Step 2. Provision resources for multi-T0. 

Multi T0 requires a minimum of four Edge Nodes: 2 Edge Nodes per T0 operating in active/standby mode. There is one shared T0 attached to the PKS mgmt plane and 1 T0 per customer/tenant -- each one takes 2 Edge Nodes minimum. See slides Page 16

The formula for determining the minimum number of Edge Nodes is 2 + (# of tenants x 2). For example, for three customers/tenants you need 8 Edge Nodes; for 10 you need 22.

### Step 3. All Edge Nodes must be interconnected by a dedicated network provisioned on the physical infrastructure. This network is used to transport traffic across the T0s.

To configure this network, you must provision a vLAN and a vLAN transport zone, or a pNIC.

A. Once you physically interconnect the T0s, go to NSX MAnager.
B. Create NSX Network and switch that uses vLAN on the existing VLAN transport zone.
C. Allocate internal private CIDR range /24 to be used for T0 communication. Each T0 router will need have a unique IP address allocated from that range. For example: 50.0.0/24


### Step 4. Configure T0 switch. 

Assuming T0 already existing (shared T0):

A. Go go Routings > Configuration Router Ports > Login Switch > inter-t0-vlan-switch. 
B. Give IP address from the allocated range. 50.0.0.1/24.

### Step 5. Provision each customer T0. 

Create T0 for each customer/tenant you want to create. Make sure it active/passive.

### Step 6. Create Uplink interface for external connectivity. 

This defines how the shared T0 connects to the customer network, most likely will be a dedicated vLAN on the vLAN transport zone.

### Step 7. Create Second Uplink Interface 

Create second uplink interface connect to the inter-t0 logical switch. Give IP address from allocated pool. One is for shared T0 network, the other for the customer / tenant network where PKS clusters will be deployed. 

### Step 8. Add Static Routes

Slide 17. Add Static routes on each T0, shared and customer. On customer T0 the default route should point to the customer network. While on Shared T0 the default route will point to the external management components like VC and NSX MAnager and internet connectivity. 

### Step 9. Add NAT Rules

On each customer/tenant T0, add NO_SNAT rules to allow global routable connectivity from Kubernetes node networks to PKS, NSX MAnager,VC components. PKS will be deployed with Node NEtwork block. Use that IP block here to configure the NO_SNAT rule.

### Step 10. Configure BGP on Customer T0. 

a. Prereq: Assign unique AS to each customer T0. You pick, must be below 64K (65 - 64)
b. Configure Route Redistribution, IP Prefix Lists, and BGP Peer.
c. Route Distribution: Create new, Select STatic, NSX Connected, NSX STatic. 
d. IP Prefix Lists: Permit rule and Deny rule. 
Permit will allow redistribution of node network with the node IP block /24 (greater than or lower than). Deny rule: Everything else is deny so 0.0.0.0/0
e. Add BGP Peer. Go to Routing > BGP > Add.

Neighbor Address: IP address of the Shared T0.
Local Address: Make sure All Uplinks is selected.
Address Families: Enter IP4_Unicast, State - Enabled, Out Filter, select the filter created in #2. Enter Remote AS number. Each T0 will have a unique AS number. 
After creating the neighbor, select BGP Edit and Enable BGP.

### Step 11. Configure BGP on Shared T0.

Repeat same steps as Customer T0. Additionally, the IP Prefix List needs to include the networks where Vcenter, NSX manager, and PKS management cluster are deployed.
Test Base Configuration
Checkpoint/verify the base configuration.

Download the routing table for each of the deployed T0s. 
Make sure that the Customer T0 routing table includes all BGP routes ("b") to reach vC, NSX Mgr, PKS mgmt plane.

Shared T0 will not have any BGP routes at this point. It will only show BGP routes when you deploy clusters.

## <a id="security-config"></a> Security Configuration

Security configuration involves advanced configurations for securing the customer/tenant networks. There are two configurations to secure:
- Traffic between tenants/customers.
- Traffic between clusters in the same tenant.

### Secure Inter-Tenant Communications
Secure each tenant so that the tenant cannot communicate and the traffic between tenants and shared T0 is restricted the legit traffic path.

#### Step 1. Define IP Sets. Inventory > Groups > IP Sets. 

Define 1 IP set to include the IP Addresses for BOSH, NSX Manager, and vCenter.
Create a 2nd IP Set to specify the inter-t0 CIDR range created previously (see above).
Create 3rd IP set: to specify the node network CIDR range.
Create 4th IP Set to specify the PKS Management Network.

#### Step 2. On each Customer T0 router, go to Service > Edge Firewall. 

Click on the default Layer 3 section and add a section/.
Name: 
State: Stateful

#### Step 3. Add firewall rules to the section.

Select the section and then add rule.

Rule 1: BGP, soure = IP Set that you created for Inter T0 cidr range, Destinationwill be the same; action = allow; services = any; apply to the Inter T0 Uplink.

Rule 2: Add Rule > Name: Node Network to Management; Source will be the IP set for the Node Network; Destination will be the IP set for VC, NSX MGr, BOSH; service = any; action = allow; apply to Inter-T0-Uplink.

Rule 3: Add Rule: Name: PKS-to-Node-Network; Source = IP Set for PKS Mgmt; Dest = Node Network IP Set; service = any; action = allow; apply to Inter-T0-Uplink.

Rule 4: Deny All = deny everything else: Source = Any; Dest = Any; Sercice = Any; Action = Drop; Apply to = Inter-T0-Uplink.

### Secure Inter-Cluster Communications
Secure communication between clusters in the same tenancy. To disallow any form of communication between Kubernetes clusters created by PKS, we can provision Security Groups and distributed firewall (DFW) rules to secure inter-cluster communications. 

NOTE: You must perform global operations (first three steps) before you deploy a Kubernetes cluster to the customer T0.

#### Step 1. Create NS group for all PKS Clusters.

This only needs to be done once at the beginning of securing inter-cluster communications.

Go to Inventory > Groups > Groups and Add new group. 
Name: All-PKS-Clusters.
Scope = Equals pks/clusters 
Scope = Equals ncp/cluster

#### Step 2. Create DFW Section.

Before you create distributed firewall rules, you must create a DFW section for the DFW rule set you define later. NOTE: You must create the DFW Section before you create a K8 cluster.

Go to Security > DFW and click on the highest rule and click Add Section Above.
Name = pks-dfw
Leave all else as defaults.
Click Manage Tags.
Add tag.
Tag name: "top" (must be precise)
Scope: "ncp/fw_sect_marker" (name must be precise)

#### Step 3. Create Deny Everything Else DFW rule.

This is a global deny rule. You will define acceptable routes later in the configuration process.

Go to Add Rule
Source: pks-all-clusters nsgroup 
Dest: pks-all-clusters nsgroup
Service: Any
Apply To: cluster-UUID-node-pods nsgroup
Action: Deny

#### Step 4. Retrieve the ID of the Kube cluster you want to secure. 

pks cli > pks cluster <clsuter-name>. 
Get the cluster UUID, which you can use to create NS Groups.

#### Step 5. Create NS Groups

a. Create NS group for cluster nodes.

Go to Inventory > Groups > Groups and Add new group. 
Name: Cluster UUID or Cluster NAME (must be unique) and append "-nodes" to the end of the name to distinguish it.
Membership Criteria: Logical Switch
Tag = Equals
Enter Cluster UUID 
Scope = Equals
Value = pks/cluster

b. Create NS group for cluster pods.

Go to Inventory > Groups > Groups and Add new group. 
Name: Cluster UUID or Cluster NAME (must be unique) and append "-pods" to the end of the name to distinguish it.
Membership Criteria: Logical Port
Tag = Equals "pks-cluster-UUID"
Scope = Equals
Value = ncp/cluster

c. Create NS group for cluster nodes and pods. This is for everything in the cluster.

Go to Inventory > Groups > Groups and Add new group. 
Name: Cluster UUID or Cluster NAME (must be unique) and append "-nodes-pods" to the end of the name to distinguish it.
Membership Criteria: Logical Switch and Logical Port (both criteria that you entered before)
Repeat the same as above for each criteria

#### Step 6. Create DFW Rules.

You need 3 rules. You already created one (the global deny all DFW rule); here are the others: 

a. Rule 1: Disable POD to NODE Communication.

Pods should not be allowed to talk to nodes. (But nodes can talk to pods.)

Go to Add Rule
Source: Pick the cluster-UUID-"node" nsgroup 
Dest: cluster-UUID-"pod" nsgroup
Service: Any
Apply To: cluster-UUID-node-pods nsgroup
Action: Deny

b. Allow Node to Node and Pods

Go to Add Rule
Source: Pick the cluster-UUID-"node" nsgroup 
Dest: cluster-UUID-"node-pods" nsgroup
Service: Any
Apply To: cluster-UUID-node-pods nsgroup
Action: Allow
