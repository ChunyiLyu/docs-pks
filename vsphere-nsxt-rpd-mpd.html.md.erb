---
title: System Requirements for Recommended and Minimum Production Deployments of PKS on vSphere with NSX-T
owner: PKS with NSX-T
---

<strong><%= modified_date %></strong>

This topic describes the recommended and minimum resource requirements for production deployments of PKS on vSphere with NSX-T. 

## vSphere Cluster Allocation

Management Cluster
- vCenter (1 VM)
- NSX-T Manager (1 VM)
- NSX-T Controllers (3 VMs)

Edge Cluster
- NSX-T Edge Nodes (2 VMs in active/standby mode)
- Requires Intel based hardware only because of DPDK requirement)

Compute Cluster
- Kubernetes cluster nodes
- Kubernetes master nodes (3 VMs)

PKS Management Plane
- Consists of Ops Manager, Bosh, PKS Control Plane VM and Harbor VMs
- Can reside in the Management cluster or Compute cluster depending on design choice. For more information, see [NSX-T toplogies for PKS](#nsxt-topologies.html). 

## vSphere Cluster Configuration 

For all vSphere clusters:
- vSphere DRS is turned on (partially automated or fully automated).
- vSphere HA
	- HA with Admission Control is turned on (with support for 1 host down max)
	- Host failure: Restart VMs
	- Admission Control: Host failures cluster tolerates = 1

##<a id='pks-rpd'></a> RPD for PKS on vSphere with NSX-T

The recommended production deployment (RPD) topology represents the VMware-recommended configuration to run PKS with NSX-T in production. The recommendations differ depending on whether you are using vSAN or not.

###<a id='pks-rpd-vsan'></a> RPD for PKS with vSAN

The recommended production deployment for PKS on a vSAN-based topology requires 12 ESXi hosts deployed and configured as follows:

- Management/Edge cluster
	- Collapsed Management/Edge cluster with 3 ESXi hosts.
	- Each ESXi host runs 1 NSX-T controller (NSX-T control plane with 3 NSX-T controllers total).
	- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- Compute clusters
	- Each Compute cluster has 3 ESXi hosts and is bound by a distinct availability zone (AZ) defined in BOSH Director.
		- Compute cluster1 (AZ1) with 3 ESXi hosts.
		- Compute cluster2 (AZ2) with 3 ESXi hosts.
		- Compute cluster3 (AZ3) with 3 ESXi hosts.
	- Each Compute cluster runs 1 instance of a PKS-provisioned Kubernetes cluster with 3 master nodes per cluster and a per-plan number of worker nodes.
- Storage
	- Each Compute cluster is backed by a vSAN datastore
	- An external shared datastore (using NFS or iSCSI for instance) must be provided to store Kubernetes Pod PV (Persistent Volumes).
	- 3 ESXi hosts are required per Compute cluster because of the vSAN cluster requirements. For data protection, vSAN creates two copies of the data and requires one witness. For more information, see <a href="./vsphere-persistent-storage.html">PersistentVolume Storage Options on vSphere</a>.
- Growth
	- The collapsed Management/Edge cluster can expand up to 64 ESXi hosts for future needs.
	- Each Compute cluster can expand up to 64 ESXi hosts for future needs.

<img src="images/nsxt/pks-vs-nsxt-rpd-vsan.png" alt="RPD for PKS with vSAN">

<p class="note"><strong>Note:</strong> </p>

###<a id='pks-rpd-no-vsan'></a> RPD for PKS without vSAN

RPD Topology (non VSAN-based) description:

- Management/Edge cluster
	- Collapsed Management/Edge cluster with 3 ESXi hosts.
	- Each ESXi host runs 1 NSX-T controller (NSX-T control plane with 3 NSX-T controllers total).
	- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- Compute clusters
	- Each Compute cluster has 2 ESXi hosts and is bound by a distinct availability zone (AZ) defined in BOSH Director.
		- Compute cluster1 (AZ1) with 2 ESXi hosts.
		- Compute cluster2 (AZ2) with 2 ESXi hosts.
		- Compute cluster3 (AZ3) with 2 ESXi hosts.
	- Each Compute cluster runs 1 instance of a PKS-provisioned Kubernetes cluster with 3 master nodes per cluster and a per-plan number of worker nodes.
- Storage
	- All Compute clusters are connected to same shared datastore (used for VM disks and Kubernetes Pod PV (Persistent Volumes). 
	- All datastores can be collapses to single datastore if needed.
- Growth
	- The collapsed Management/Edge cluster can expand up to 64 ESXi hosts for future needs.
	- Each Compute cluster can expand up to 64 ESXi hosts for future needs

<img src="images/nsxt/pks-vs-nsxt-rpd.png" alt="RPD for PKS without vSAN">

##<a id='pks-mpd'></a> MPD for PKS

The minimum production deployment (MPD) topology represents the VMware supported minimum deployment to run PKS/NSX-T in production. 

###<a id='pks-mpd-toplogy'></a> MPD Topology

<p class="note"><strong>Note:</strong> The MPD topology for PKS applies to vSAN and non-vSAN configuration.</p>

The MPD topology for PKS requires the following minimum configuration: 

- A single collapsed Management/Edge/Compute cluster using 3 ESXi hosts in total. 
- Each ESXi host runs 1 NSX-T Controller (NSX-T control plane with 3 NSX-T controllers total).
- Each ESXi host runs 1 Kubernetes master node (Kubernetes cluster with 3 master nodes total).
- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- The shared datastore (NFS/SCSI/â€¦) or vSAN datastore will be leveraged for VM disks and Kubernetes Pod PV (Persistent Volumes).
- The collapsed Management/Edge/Compute cluster can expand up to 64 ESXi hosts for future needs.

###<a id='pks-mpd-toplogy'></a> MPD Considerations

Keep in mind the following considerations when using MPD:

- When deploying 1 NSX-T Controller per ESXi host, create a DRS anti-affinity rule of type "separate virtual machines" for each of the 3 controllers.
- When deploying 2 NSX-T Edge Nodes across 2 different ESXi hosts, create DRS anti-affinity rule of type "separate virtual machines" for the 2 VMs.
- MPD topology (Base): Make sure to leverage vSphere resource pools to allocate proper HW resources for management plane components (tune reservation and limits accordingly).
- There is no fault tolerance for the Kubernetes cluster because PKS Availability Zones are not fully leveraged with this topology.
- PKS AZ can be mapped as vSphere Resource Pool.
- After deploying the Kubernetes cluster, user has to intervene to make sure each master node is living on a different ESXi host (by tuning the DRS anti-affinity rule of type "separate virtual machines" for instance).
 
<img src="images/nsxt/pks-vs-nsxt-mpd.png" alt="MPD for PKS">