---
title: vSphere Persistent Volume Storage Options and Workflow
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes options for configuring PKS on vSphere to support stateful applications using persistent volumes (PV).

##<a id='pv-kubernetes'></a> Requirements for Running Stateful Applications in Kubernetes

There are several requirements to consider when running stateful applications in Kubernetes:

- Pods are ephemeral by nature; data that needs to be persisted must be accessible on restart and rescheduling of a pod.
- When a pod is rescheduled, it may be on a different host. Storage must be shifted and made available on the new host for the pod to start gracefully.
- The application should not have to worry about the volume and data. The underlying infrastructure should handle the complexity of unmounting and mounting.
- Certain applications have a strong sense of identity and the disk used by a container with certain identity is tied to it. If a pod with a certain ID gets rescheduled, the disk associated with that ID must be reattached to the new pod instance.

##<a id='vsphere-pv-scenarios'></a> Persistent Volume Provisioning Support in Kubernetes

Kubernetes provides two ways to provision persistent storage for stateful applications: 

- **Static provisioning**: Kubernetes administrator creates Virtual Machine Disk (VMDK) and Persistent Volumes (PVs). Developers issue Persistent Volume Claims (PVCs) on the pre-defined PVs.
- **Dynamic provisioning**: Developers issue PV Claims against a Storage Class (SC) object. The provisioning of the persistent storage depends on the infrastructure. With PKS on vSphere, the vSphere Cloud Provider (VCP) automatically provisions the VMDK and PVs.

For more information about persistent volumes in Kubernetes, refer to the [Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).

PVs can be used with two types of Kubernetes workloads:

- [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) (Pod)
- [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)

<p class="note"><strong>Note</strong>: This topic assumes strong famialiarity with persistent volumes and workloads in Kubernetes.

##<a id='vsphere-pv-support'></a> vSphere Support for Static and Dynamic PVs

With PKS on vSphere there are two storage options from which to choose to support stateful applications: vSAN datastores and VMFS over NFS (Network File Share), iSCSI (Internet Small Computer Systems Interface), or fiber channel (FC) datastores. Refer to the [vSAN documentation](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.virtualsan.doc/GUID-AEF15062-1ED9-4E2B-BA12-A5CE0932B976.html) and the [VMFS documentation](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.storage.doc/GUID-5EE84941-366D-4D37-8B7B-767D08928888.html?hWord=N4IghgNiBcIG4FsBmBnEBfIA) for more information on these storage options.

<p class="note"><strong>Note</strong>: This topic assumes strong famialiarity vSAN and VMFS storage technologies on the vSphere platform.

In PKS an availability zone (AZ) corresponds to a vSphere cluster or resource pool. A resource pool (RP) is a vSphere construct that is not linked to a particular ESXi host. The vSAN datastore boundary is delimited by the vSphere cluster. All ESXi hosts in the same vSphere Cluster belong to same vSAN datastore. ESXi hosts in a different vSphere Cluster belong to a different vSAN datastore. Each vSphere cluster has its own vSAN datastore(s).

The table below summarizes PKS support for PVs in Kubernetes when deployed on vSphere. This information assumes that the underlying vSphere infrastructure is a single locality environment, that is, all vSphere compute clusters are closed in term of distance from one to the others. It does not apply to multi-site or stretched cluster configurations (for vSAN).

<table>
  <tr>
    <th>Storage Mechanism</th>
    <th>vSAN datastore(s)</th>
    <th>VMFS over NFS/iSCSI/FC datastore(s)</th>
  </tr>
  <tr>
    <td>Single vSphere compute cluster (single AZ or multiple AZs using RPs) with local datastore</td>
    <td>Both static and dynamic PV provisioning are suppported.</td>
    <td>Both static and dynamic PV provisioning are suppported.</td>
  </tr>
  <tr>
    <td>Multiple vSphere compute clusters (multiple AZs) with local datastore</td>
    <td>Neither static nor dynamic PV provisioning are suppported.</td>
    <td>Neither static nor dynamic PV provisioning are suppported.</td>
  </tr>
  <tr>
    <td>Multiple vSphere compute clusters (multiple AZs) with shared datastore</td>
    <td>vSAN does not support shared datastore across vSphere Clusters.</td>
    <td>Both static and dynamic PV provisioning are suppported.</td>
  </tr>  
</table>

##<a id='vsphere-pv-a'></a> Single vSphere Compute Cluster with Local Datastore(s)

This section describes PKS support for PVs in with a single vSphere compute cluster (single AZ or multiple AZs if using RPs) with a datastore that is local to the single vSphere compute cluster.

### Single vSphere Compute Cluster with vSAN Datastore

The following diagram illustrates a vSphere environment with a single compute cluster (single AZ or multiple AZs using resource pools) and a vSAN datastore local to the vSphere compute cluster. For this toplogy PKS supports both static and dynamic PV provisioning. The latter is recommended.

  <img src="images/vsphere/vsphere-pv-a1.png" alt="Single vSphere compute cluster with local vSAN datastore">

In this toplogy, a single vSphere compute cluster hosts all Kubenetes clusters. vSAN is enabled on the compute cluster which exposes a single unique vSAN datastore (`vSAN datastore1` in the diagram).

One or multiple AZs can be instantiated on top of the compute cluster. With this configuration, one or more AZs are mapped to vSphere resource pools. The AZ is not bound to a failure domain because its resource pool is not linked to a particular ESXi host.

With this toplogy you can create multiple vSAN datastores (on top of same compute cluster) using different disk groups on each ESXi host. PVs (backed by respective VMDK files) can be dispatched across the datastores to mitigate the impact of datastore failure. For StatefulSets, all PVs used by different instances of replica will land in the same datastore.

Failure scenarios for this toplogy:
- Disk(s) on ESXi host(s) are down (within the limit of the vSAN `failure to tolerate` value): no impact on PV.
- ESXi host(s) are down (within the limit of the vSAN `failure to tolerate` value): no impact on PV.
- Datastore is down: PVs on the down datastore are unreachable.

### Single vSphere Compute Cluster with VMFS Datastore

The following diagram illustrates a vSphere environment with a single vSphere compute cluster and a shared datastore using VMFS over NFS, iSCSI, or FC. For this toplogy PKS supports both static and dynamic PV provisioning. The latter is recommended. 

  <img src="images/vsphere/vsphere-pv-a2.png" alt="Single vSphere compute cluster with shared VMFS datastore">

In this topology, a single vSphere compute cluster hosts all Kubernetes clusters. The shared datastore is used with the compute cluster ("Shared Datastore1" in the diagram).

One or multiple AZs can be instantiated on top of the compute cluster. With this configuration, one or more AZs are mapped to vSphere resource pools. The AZ is not bound to a failure domain because its resource pool is not linked to a particular ESXi host.

With this toplogy you can create multiple shared datastores connected to the same compute cluster. PVs (backed by respective VMDK files) can be dispatched across the datastores to mitigate the impact of datastore failure. For StatefulSets, all PVs used by different instances of replica will land in the same datastore.

Failure scenarios for this toplogy:
- ESXi host(s) are down: No impact on PVs.
- Datastore is down: PVs on the down datastore are unreachable.

##<a id='vsphere-pv-b'></a> Multiple vSphere Compute Clusters with Datastore(s) Local to Each vSphere Compute Cluster

This section describes PKS support for PVs in an environment with multiple vSphere compute clusters (multiple AZs) with datastore(s) local to each vSphere compute cluster.

### Multiple vSphere Compute Clusters with Local vSAN Datastores

The following diagram illustrates a vSphere environment with mutliple vSphere compute clusters (mutliple AZs) with vSAN datastore(s) local to each vSphere compute cluster. The current release of PKS does not support the use of PVs with this topology.

  <img src="images/vsphere/vsphere-pv-b1-vsan.png" alt="Multi vSphere compute clusters (Multi AZs) with local vSAN datastore">

In this toplogy, vSAN is enabled on each compute cluster. There is one local vSAN datastore per compute cluster (in the diagram, vSAN datastore1 is provisioned for Compute Cluster 1, vSAN datastore2 is provisioned for Compute Cluster 2, etc.). One or multiple AZ can be instantiated. Each AZ is mapped to a vSphere compute cluster. The AZ is bound to a failure domain which is typically the physical rack where the compute cluster is hosted.

### Multiple vSphere Compute Clusters with Local VMFS over NFS/iSCSI/FC Datastores

The following diagram illustrates a vSphere environment with mutliple vSphere compute clusters (mutliple AZs) with VMFS over NFS/iSCSI/FC datastore(s) local to each vSphere compute cluster. The current release of PKS does not support the use of PVs with this topology.

  <img src="images/vsphere/vsphere-pv-b2-vmfs.png" alt="Multi vSphere compute clusters (Multi AZs) with local VMFS datastore">

In this topology, multiple vSphere compute clusters are used to host all Kubernetes clusters. A unique shared datastore (NFS/iSCSI/FC) is used per vSphere compute cluster (in the diagram, Shared Datastore1 is connected to Compute Cluster 1, Shared Datastore2 is connected to Compute Cluster 2, etc.). One or multiple AZs can be instantiated. Each AZ is mapped to a vSphere compute cluste. The AZ is bound to a failure domain which is typically the physical rack where the compute cluster is hosted.

##<a id='vsphere-pv-c'></a> Multiple vSphere Compute Clusters with Shared Datastore across All Compute Clusters

This section describes PKS support for vSphere environments with multiple compute clusters (mutliple AZs) with datastores shared across all vSphere compute clusters.

### Multiple vSphere Compute Clusters with Shared vSAN Datastores across All Compute Clusters

With this topology each vSAN datastore is only visible per vSphere compute cluster. Consequently, it is not possible to have a vSAN datastore shared across all vSphere compute clusters. As a result, neither static nor dynamic PV provisioning is supported with this toplogy. 

However, as illustrated in the diagram below, if a shared datastore (NFS/iSCSI/FC) is inserted across all vSAN-based vSphere compute clusters, this topology would support both static and dynamic PV provisioning.

  <img src="images/vsphere/vsphere-pv-c1-vsan.png" alt="Multi vSphere compute clusters (Multi AZs) with Shared vSAN datastore">

### Multiple vSphere Compute Clusters with Shared VMFS over NFS/iSCSI/FC Datastores across All Compute Clusters

The following diagram illustrates a vSphere environment with multiple compute clusters (mutliple AZs) VMFS over NFS/iSCSI/FC datastores shared across all vSphere compute clusters. For this toplogy, PKS supports both static and dynamic PV provisioning. The latter is recommended.

  <img src="images/vsphere/vsphere-pv-c2-vmfs.png" alt="Multi vSphere compute clusters (Multi AZs) with Shared VMFS datastore">

In this topology, multiple vSphere compute clusters are used to host all Kubernetes clusters. A uniqueshared datastore (VMFS over NFS/iSCSI/FC) is used across all compute clusters (Shared Datastore1 in the diagram). One or multiple AZs can be instantiated. Each AZ is mapped to a compute cluster. The AZ is bound to a failure domain which is typically the physical rack where the compute cluster is hosted.

You can have multiple shared datastores connected across all the vSphere compute clusters. PVs (backed by respective VMDK files) can then be dispatched across those datastores to mitigate the impact of datastore failure. For StatefulSets, all PVs used by different instances of replica will land in the same datastore.

Failover scenarios for this toplogy:
- ESXi host(s) are down: No impact on PVs.
- 1 Shared datastore is down: PVs on the down datastore are unreachable.

##<a id='pv-workflows'></a> PV Provioning Workflows for Supported Toplogies

This section describes how to provision both static and dynamic PVs for supported toplogies.

For static PV provisioning, there is no need to specify a Storage Class (SC). Consequently, the persisten volume claim (PVC) does not need to reference any SC. For dynamic PV provisioning, you must specify a Storage Class (SC) and define the PVC using a reference to that SC. 

### Static PV Provisioning Workflow

For static PV provisioning, the procedure is to manually create a VMDK file that will be used as a storage backend for the PV. When the PV is created, Kubernetes knows which volume instance (with associated capacity) is ready for use. When a PVC or VolumeClaimTemplate (for StatefulSets) is requested, Kubernetes will choose an available PV in the system and allocate it to the Pod or StatefulSets.

#### Static PV Provisioning for Deployment Workloads
 
For the **Deployment** (Pod) workload with static PV provisioning, the procedure is as follows:

1. Create VMDK files:

```
[root@ESXi-1:~] cd /vmfs
[root@ESXi-1:/vmfs] cd volumes/
[root@ESXi-1:/vmfs/volumes] cd <datastore>/
[root@ESXi-1:/vmfs/volumes/7e6c0ca3-8c4873ed] cd kubevols/
[root@ESXi-1:/vmfs/volumes/7e6c0ca3-8c4873ed/kubevols] vmkfstools -c 2G redis-master.vmdk
```

1. Define PV using a YAML manifest file (for example, `redis-master-pv.yaml`) containing a reference to the VMDK file:

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-master-pv
spec:
  capacity:
	storage: 2Gi
  accessModes:
	- ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  vsphereVolume:
	volumePath: "[NFS-LAB-DATASTORE] kubevols/redis-master"
	fsType: ext4
```

1. Define PVC using a YAML manifest file (for example, `redis-master-claim.yaml`):

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: redis-master-claim
spec:
  accessModes:
	- ReadWriteOnce
  resources:
	requests:
  	storage: 2Gi
```

1. Define a deployment using a YAML manifest file (for example, `redis-master.yaml`) that references the PVC:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
…
spec:
 template:
   spec:
     volumes:
  	- name: redis-master-data
    	persistentVolumeClaim:
      	claimName: redis-master-claim
```

#### Static PV Provisioning for StatefulSets Workloads 

For the **StatefulSets** (with 3 replicas) workload with static PV provisioning, the procedure is as follows:

1. Create VMDK files, for example:

```
[root@ESXi-1:~] cd /vmfs
[root@ESXi-1:/vmfs] cd volumes/
[root@ESXi-1:/vmfs/volumes] cd <datastore>/
[root@ESXi-1:/vmfs/volumes/7e6c0ca3-8c4873ed] cd kubevols/
[root@ESXi-1:/vmfs/volumes/7e6c0ca3-8c4873ed/kubevols] vmkfstools -c 10G mysql-pv-1.vmdk
[root@ESXi-1:/vmfs/volumes/7e6c0ca3-8c4873ed/kubevols] vmkfstools -c 10G mysql-pv-2.vmdk
[root@ESXi-1:/vmfs/volumes/7e6c0ca3-8c4873ed/kubevols] vmkfstools -c 10G mysql-pv-3.vmdk
```

1. Define PV using a YAML manifest file (for example, `mysql-pv-1.yaml`) containing a reference to the VMDK file:

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv-1
spec:
  capacity:
	storage: 10Gi
  accessModes:
	- ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  vsphereVolume:
    volumePath: "[NFS-LAB-DATASTORE] kubevols/mysql-pv-1"
	fsType: ext4

1. Define PV using a YAML manifest file (for example, `mysql-pv-2.yaml`) containing a reference to the VMDK file:

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv-2
spec:
  capacity:
	storage: 10Gi
  accessModes:
	- ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  vsphereVolume:
    volumePath: "[NFS-LAB-DATASTORE] kubevols/mysql-pv-2"
	fsType: ext4
```

1. Define PV using a YAML manifest file (for example, `mysql-pv-3.yaml`) containing a reference to the VMDK file:

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv-3
spec:
  capacity:
	storage: 10Gi
  accessModes:
	- ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  vsphereVolume:
    volumePath: "[NFS-LAB-DATASTORE] kubevols/mysql-pv-3"
	fsType: ext4
```

1. Define a StatefultSets object using a YAML manifest file (for example, `mysql-statefulsets.yaml`):

```
piVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
...
volumeClaimTemplates:
  - metadata:
  	name: data
	spec:
  	accessModes: ["ReadWriteOnce"]
  	resources:
    	requests:
      	storage: 10Gi
```

<p class="note"><strong>Note</strong>: In the volumeClaimTemplates, you must specify the required storage size for each replica; you do **not** to refer to any storage class.

### Dynamic PV Provisioning Workflow

For dynamic PV provisioning, the procedure is to define and create a PVC that will automatically (dynamically) trigger the creation of the PV and its backend VMDK file. When the PV is created, Kubernetes knows which volume instance (with associated capacity) is available for use. When a PVC or VolumeClaimTemplate (for StatefulSets) is requested, Kubernetes will choose an available PV and allocate it to the Pod deployment or StatefulSets workload.

#### Dynamic PV Provisioning for Deployment Workloads

For the **Deployment** (Pod) workload with dynamic PV provisioning, the procedure is as follows:

1. Define Storage Class (SC) using a YAML manifest file (for example, `redis-sc.yaml`):

```
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: thin-disk
provisioner: kubernetes.io/vsphere-volume
parameters:
	datastore: Datastore-NFS-VM
	diskformat: thin
	fstype: ext3
```

1. Define PVC using a YAML manifest file (for example, `redis-master-claim.yaml`) where the PVC references the SC:

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: redis-master-claim
  annotations:
	volume.beta.kubernetes.io/storage-class: thin-disk
spec:
  accessModes:
	- ReadWriteOnce
  resources:
	requests:
  	storage: 2Gi
```

<p class="note"><strong>Note</strong>: When the PVC is deployed, the PV (and associated VMDK file) is automatically created by the vSphere Cloud Provider plugin.

1. Define a Pod deployment using YAML manifest file (for example, `redis-master.yaml`) that references the PVC:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
…
spec:
 template:
   spec:
     volumes:
  	- name: redis-master-data
    	persistentVolumeClaim:
      	claimName: redis-master-claim
```

#### Dynamic PV Provisioning for StatefulSets Workloads

For the **StatefulSets** (with 3 replicas) workload with dynamic PV provisioning, the procedure is as follows:

1. Define the Storage Class (SC) using a YAML manifest file (for example, `mysql-sc.yaml`):

```
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: my-storage-class
provisioner: kubernetes.io/vsphere-volume
parameters:
	datastore: Datastore-NFS-VM
	diskformat: thin
	fstype: ext3
```

1. Define a StatefultSets object using a YAML manifest file (for example, `mysql-statefulsets.yaml`):

```
piVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
...
volumeClaimTemplates:
  - metadata:
  	name: data
	spec:
  	accessModes: ["ReadWriteOnce"]
      storageClassName: "my-storage-class"
  	resources:
    	requests:
      	storage: 10Gi
```

<p class="note"><strong>Note</strong>: In the volumeClaimTemplates, specify the required storage size for each replica. Unlike static provisioning, with dymnamic PV provisioning case you must explicitly refer to the desired Storage Class (SC).