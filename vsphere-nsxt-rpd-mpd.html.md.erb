---
title: Hardware Requirements for Production Deployments of PKS on vSphere with NSX-T
owner: PKS with NSX-T
---

<strong><%= modified_date %></strong>

This topic describes the hardware requirements for production deployments of PKS on vSphere with NSX-T, including recommended and minimum production deployment guidelines.

## vSphere Cluster Characteristics for PKS

The PKS Management Cluster on vSphere comprises the following components:

- vCenter Server
- NSX-T Manager
- NSX-T Controller (quantity 3)

The PKS Edge Cluster on vSphere comprises the following components:
- NSX-T Edge Node (quantity 2)

<p class="note"><strong>Note</strong>: NSX-T Edge Nodes must be deployed on Intel-based hardware.</p>

The PKS Compute Cluster on vSphere comprises the following components:
- Kubernetes master nodes (quantity 3)
- Kubernetes worker nodes

The PKS Management Plane comprises the following components:
- Pivotal Ops Manager
- Pivotal BOSH Director 
- PKS Control Plane
- VMware Harbor Registry

Depending on your design choice, the PKS management components can be deployed in the PKS Management Cluster or in the PKS Compute Cluster. For more information, see [NSX-T toplogies for PKS](#nsxt-topologies.html). 

In addion, for all vSphere clusters, the following configurations are required:
- The vSphere Distributed Resource Scheduler (DRS) is turned on (partially automated or fully automated). 
- vSphere high-availiabity (HA) with Admission Control (AC) to support 1 ESXi host failure is turned on and configured as follows:
	- Host failure: Restart VMs
	- Admission Control: Host failures cluster tolerates = 1

##<a id='pks-rpd'></a> RPD for PKS on vSphere with NSX-T

The recommended production deployment (RPD) topology represents the VMware-recommended configuration to run PKS with NSX-T in production. The recommendations differ depending on whether you are using vSAN or not.

###<a id='pks-rpd-vsan'></a> RPD for PKS with vSAN

The recommended production deployment for PKS on a vSAN-based topology requires 12 ESXi hosts deployed and configured as follows:

- Management/Edge cluster
	- Collapsed Management/Edge cluster with 3 ESXi hosts.
	- Each ESXi host runs 1 NSX-T controller (NSX-T control plane with 3 NSX-T controllers total).
	- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- Compute clusters
	- Each Compute cluster has 3 ESXi hosts and is bound by a distinct availability zone (AZ) defined in BOSH Director.
		- Compute cluster1 (AZ1) with 3 ESXi hosts.
		- Compute cluster2 (AZ2) with 3 ESXi hosts.
		- Compute cluster3 (AZ3) with 3 ESXi hosts.
	- Each Compute cluster runs 1 instance of a PKS-provisioned Kubernetes cluster with 3 master nodes per cluster and a per-plan number of worker nodes.
- Storage
	- Each Compute cluster is backed by a vSAN datastore
	- An external shared datastore (using NFS or iSCSI for instance) must be provided to store Kubernetes Pod PV (Persistent Volumes).
	- 3 ESXi hosts are required per Compute cluster because of the vSAN cluster requirements. For data protection, vSAN creates two copies of the data and requires one witness. For more information, see <a href="./vsphere-persistent-storage.html">PersistentVolume Storage Options on vSphere</a>.
- Growth
	- The collapsed Management/Edge cluster can expand up to 64 ESXi hosts for future needs.
	- Each Compute cluster can expand up to 64 ESXi hosts for future needs.

<img src="images/nsxt/pks-vs-nsxt-rpd-vsan.png" alt="RPD for PKS with vSAN">

###<a id='pks-rpd-no-vsan'></a> RPD for PKS without vSAN

RPD Topology (non VSAN-based) description:

- Management/Edge cluster
	- Collapsed Management/Edge cluster with 3 ESXi hosts.
	- Each ESXi host runs 1 NSX-T controller (NSX-T control plane with 3 NSX-T controllers total).
	- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- Compute clusters
	- Each Compute cluster has 2 ESXi hosts and is bound by a distinct availability zone (AZ) defined in BOSH Director.
		- Compute cluster1 (AZ1) with 2 ESXi hosts.
		- Compute cluster2 (AZ2) with 2 ESXi hosts.
		- Compute cluster3 (AZ3) with 2 ESXi hosts.
	- Each Compute cluster runs 1 instance of a PKS-provisioned Kubernetes cluster with 3 master nodes per cluster and a per-plan number of worker nodes.
- Storage
	- All Compute clusters are connected to same shared datastore (used for VM disks and Kubernetes Pod PV (Persistent Volumes). 
	- All datastores can be collapses to single datastore if needed.
- Growth
	- The collapsed Management/Edge cluster can expand up to 64 ESXi hosts for future needs.
	- Each Compute cluster can expand up to 64 ESXi hosts for future needs

<img src="images/nsxt/pks-vs-nsxt-rpd.png" alt="RPD for PKS without vSAN">

##<a id='pks-mpd'></a> MPD for PKS

The minimum production deployment (MPD) topology represents the VMware supported minimum deployment to run PKS/NSX-T in production. 

###<a id='pks-mpd-toplogy'></a> MPD Topology

<p class="note"><strong>Note:</strong> The MPD topology for PKS applies to vSAN and non-vSAN configuration.</p>

The MPD topology for PKS requires the following minimum configuration: 

- A single collapsed Management/Edge/Compute cluster using 3 ESXi hosts in total. 
- Each ESXi host runs 1 NSX-T Controller (NSX-T control plane with 3 NSX-T controllers total).
- Each ESXi host runs 1 Kubernetes master node (Kubernetes cluster with 3 master nodes total).
- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- The shared datastore (NFS/SCSI/â€¦) or vSAN datastore will be leveraged for VM disks and Kubernetes Pod PV (Persistent Volumes).
- The collapsed Management/Edge/Compute cluster can expand up to 64 ESXi hosts for future needs.

###<a id='pks-mpd-toplogy'></a> MPD Considerations

Keep in mind the following considerations when using MPD:

- When deploying 1 NSX-T Controller per ESXi host, create a DRS anti-affinity rule of type "separate virtual machines" for each of the 3 controllers.
- When deploying 2 NSX-T Edge Nodes across 2 different ESXi hosts, create DRS anti-affinity rule of type "separate virtual machines" for the 2 VMs.
- MPD topology (Base): Make sure to leverage vSphere resource pools to allocate proper HW resources for management plane components (tune reservation and limits accordingly).
- There is no fault tolerance for the Kubernetes cluster because PKS Availability Zones are not fully leveraged with this topology.
- PKS AZ can be mapped as vSphere Resource Pool.
- After deploying the Kubernetes cluster, user has to intervene to make sure each master node is living on a different ESXi host (by tuning the DRS anti-affinity rule of type "separate virtual machines" for instance).
 
<img src="images/nsxt/pks-vs-nsxt-mpd.png" alt="MPD for PKS">

## VM Size Inventory

The following tables list the VMs and their sizes for PKS on vSphere with NSX-T deployments.

### Control Plane VM Sizes

The following table lists the resource requirements for each VM in the PKS infrastructure and control plane. 

VM 						| CPU   | Memory (GB) 	| Disk Space (GB)
------------------------|-------|---------------|----------------
vCenter (small size) 	| 4 	| 16 			| 290
NSX-T Manager			| 4		| 16			| 140
NSX-T Controller 1		| 4 	| 16 			| 120
NSX-T Controller 2		| 4 	| 16 			| 120
NSX-T Controller 3		| 4 	| 16 			| 120
Ops Manager 			| 1 	| 8 			| 160
BOSH Director 			| 2 	| 8 			| 103
Harbor Registry 		| 2 	| 8 			| 167
------------------------|-------|---------------|----------------
TOTAL					| 27	| 112			| 1.25 TB

### NSX-T Edge Nodes VM Sizes

VM 						| CPU   				| Memory (GB) 	| Disk Space (GB)
------------------------|-----------------------|---------------|----------------
NSX-T Edge Node 1		| 8 (Intel CPU only)	| 16 			| 120
NSX-T Edge Node 1		| 8 (Intel CPU only)	| 16 			| 120
------------------------|-----------------------|---------------|----------------
TOTAL					| 16					| 32			| .25 TB

### Kubernetes Cluster Nodes VM Sizes

The following table lists general sizing information:

VM 						| CPU   	| Memory (GB) 	| Ephemeral Disk Space | Persistent Disk Space
------------------------|-----------|---------------|----------------------|----------------------
Master Nodes 			| 1 to 16 	| 1 to 64 		| 8 to 256 GB 		   | 1 GB to 32 TB
Worker Nodes 			| 1 to 16 	| 1 to 64 		| 8 to 256 GB 		   | 1 GB to 32 TB


The following table shows sizing information for two example Kubernetes clusters with 3 master nodes and 5 worker nodes each.

VM 						| CPU per Node 	| Memory (GB) per Node 	| Ephemeral Disk Space per Node (GB) | Persistent Disk Space per Node (GB)
------------------------|---------------|-----------------------|------------------------------------|------------------------------------
Master Nodes (6 total)	| 2 			| 8 					| 64 		   				 		 | 128
Worker Nodes (10 todal)	| 4 			| 16 					| 64 		   				 		 | 256

## Hardware Requirements

The following tables list the VMs and their sizes for PKS on vSphere with NSX-T deployments.

### MPD Hardware Requirements

The following table shows the hardware requirements for a MPD of PKS where you have a collapsed Management, Edge, and Compute cluster.

VM 					| Number of Hosts 	| Total Cores per Host 		| Memory per Host (GB)  | NICs per Host | Shared Datastore
--------------------|-------------------|---------------------------|-----------------------|---------------|-----------------
Collapsed Cluster	| 3 				| 32 (with Hyper Threading)	| 236 		   			| 2x 10GbE		| 5.9 TB


### RPD Hardware Requirements

The following table lists the hardware requirements for a RPD of PKS using vSAN.

VM 					| Number of Hosts 	| Total Cores per Host 		| Memory per Host (GB)  | NICs per Host | Shared Datastore
--------------------|-------------------|---------------------------|-----------------------|---------------|-----------------
Management Cluster	| 3 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Edge Cluster		| 2 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Compute Cluster	1	| 3 				| 6 (with Hyper Threading)	| 48 		   			| 2x 10GbE		| 192 GB
Compute Cluster	2	| 3 				| 6 (with Hyper Threading)	| 48 		   			| 2x 10GbE		| 192 GB
Compute Cluster	3	| 3 				| 6 (with Hyper Threading)	| 48 		   			| 2x 10GbE		| 192 GB

The following table lists the hardware requirements for a RPD of PKS not using vSAN.

VM 					| Number of Hosts 	| Total Cores per Host 		| Memory per Host (GB)  | NICs per Host | Shared Datastore
--------------------|-------------------|---------------------------|-----------------------|---------------|-----------------
Management Cluster	| 3 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Edge Cluster		| 2 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Compute Cluster	1	| 3 				| 10 (with Hyper Threading)	| 70 		   			| 2x 10GbE		| 192 GB
Compute Cluster	2	| 3 				| 10 (with Hyper Threading)	| 70 		   			| 2x 10GbE		| 192 GB
Compute Cluster	3	| 3 				| 10 (with Hyper Threading)	| 70 		   			| 2x 10GbE		| 192 GB