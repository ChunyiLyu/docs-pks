---
title: Hardware Requirements for Production Deployments of PKS on vSphere with NSX-T
owner: PKS with NSX-T
---

<strong><%= modified_date %></strong>

This topic describes the hardware requirements for **production** deployments of PKS on vSphere with NSX-T, including recommended and minimum deployment guidelines.

##<a id='pks-cluster-chars'></a> vSphere Cluster Characteristics for PKS

In vSphere a [cluster](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.vcenterhost.doc/GUID-F7818000-26E3-4E2A-93D2-FCDCE7114508.html?hWord=N4IghgNiBcIMYQK4GcAuBTATiAvkA) is a collection of ESXi hosts and associated virtual machines (VMs) with shared resources and a shared management interface.

Deploying PKS on vSphere with NSX-T requires the creation of a number of vSphere cluster objects, including: 

- PKS Management Cluster
- PKS Edge Cluster
- PKS Compute Cluster

###<a id='pks-mgmt-cluster'></a> PKS Management Cluster

The PKS Management Cluster on vSphere comprises the following components:

- vCenter Server
- NSX-T Manager
- NSX-T Controller (quantity 3)

###<a id='pks-edge-cluster'></a> PKS Edge Cluster

The PKS Edge Cluster on vSphere comprises two or more NSX-T Edge Nodes in active/standby mode. The minimum number of Edge Nodes per Edge Cluster is 2; the maximum is 10. PKS supports running Edge Node pairs in active/standby mode only.

<p class="note"><strong>Note</strong>: NSX-T Edge Nodes must be deployed on Intel-based hardware.</p>

###<a id='pks-compute-cluster'></a> PKS Compute Cluster

The PKS Compute Cluster on vSphere comprises the following components:

- Kubernetes master nodes (quantity 3)
- Kubernetes worker nodes (quantity depends on the chosen plan)

###<a id='pks-cluster-consids'></a> PKS Management Plane Considerations

The PKS Management Plane comprises the following components:

- Pivotal Ops Manager
- Pivotal BOSH Director 
- PKS Control Plane
- VMware Harbor Registry

Depending on your design choice, PKS management components can be deployed in the PKS Management Cluster (on the standard vSphere network) or in the PKS Compute Cluster (on the NSX-T-defined virtual network). For more information, see [NSX-T topologies for PKS](#nsxt-topologies.html). 

###<a id='pks-cluster-reqs'></a> Additional Requirements for vSphere Clusters for PKS

For each vSphere cluster defined for PKS, the following configurations are required for production workloads:

- The vSphere Distributed Resource Scheduler (DRS) is enabled. For more information, see <a href="https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.resmgmt.doc/GUID-8ACF3502-5314-469F-8CC9-4A9BD5925BC2.html">Creating a DRS Cluster</a> in the vSphere documentation.</p>

- The DRS custom automation level is set to **Partially Automated** or **Fully Automated**. For more information, see <a href="https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.resmgmt.doc/GUID-C21C0609-923B-46FB-920C-887F00DBCAB9.html">Set a Custom Automation Level for a Virtual Machine</a> in the vSphere documentation.</p> 

- vSphere high-availability (HA) is enabled. For more information, see <a href="https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.avail.doc/GUID-5432CA24-14F1-44E3-87FB-61D937831CF6.html">Creating and Using vSphere HA Clusters</a> in the vSphere documentation.</p>

- vSphere HA Admission Control (AC) is configured to support 1 ESXi host failure. Specifically:
	- Host failure: Restart VMs
	- Admission Control: Host failures cluster tolerates = 1

For more information, see <a href="https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.avail.doc/GUID-C4059DB9-D673-45CB-918C-68C87FEB060A.html">Configure Admission Control</a> in the vSphere documentation.</p>

##<a id='pks-rpd'></a> RPD for PKS on vSphere with NSX-T

The recommended production deployment (RPD) topology represents the VMware-recommended configuration to run PKS on vSphere with NSX-T in production.

<p class="note"><strong>Note</strong>: The recommendations differ depending on whether you are using vSAN or not.</p>

###<a id='pks-rpd-vsan'></a> RPD for PKS with vSAN

The recommended production deployment (RPD) for PKS on vSphere with NSX-T and vSAN requires 12 ESXi hosts. The diagram below shows the topology for this deployment scenario. 

<img src="images/nsxt/pks-vs-nsxt-rpd-vsan.png" alt="RPD for PKS with vSAN">

The following subsections describe configuration details for the RPD of PKS on vSphere with NSX-T and vSAN.

#### Management/Edge Cluster

The RPD topology for PKS with vSAN includes a Management/Edge Cluster with the following characteristics:

- Collapsed Management/Edge Cluster with 3 ESXi hosts.
- Each ESXi host runs 1 NSX-T Controller. The NSX-T Control Plane has 3 NSX-T Controllers total.
- 2 NSX-T Edge Nodes are deployed across 2 different ESXi hosts.

#### Compute Clusters

The RPD topology for PKS with vSAN includes 3 Compute Clusters with the following characteristics:

- Each Compute cluster has 3 ESXi hosts and is bound by a distinct availability zone (AZ) defined in BOSH Director.
	- Compute cluster1 (AZ1) with 3 ESXi hosts.
	- Compute cluster2 (AZ2) with 3 ESXi hosts.
	- Compute cluster3 (AZ3) with 3 ESXi hosts.
- Each Compute cluster runs 1 instance of a PKS-provisioned Kubernetes cluster with 3 master nodes per cluster and a per-plan number of worker nodes.

#### Storage (vSAN)

The RPD topology for PKS with vSAN requires the following storage configuration:

- Each Compute Cluster is backed by a vSAN datastore
- An external shared datastore (using NFS or iSCSI for instance) must be provided to store Kubernetes Pod PV (Persistent Volumes).
- 3 ESXi hosts are required per Compute cluster because of the vSAN cluster requirements. For data protection, vSAN creates two copies of the data and requires one witness. For more information, see <a href="./vsphere-persistent-storage.html">PersistentVolume Storage Options on vSphere</a>.

#### Future Growth

The RPD topology for PKS with vSAN can be scaled as follows to accommodate future growth requirements:

- The collapsed Management/Edge Cluster can be expanded to include up to 64 ESXi hosts.
- Each Compute Cluster can expand up to 64 ESXi hosts.

###<a id='pks-rpd-no-vsan'></a> RPD for PKS without vSAN

The recommended production deployment (RPD) for PKS on vSphere with NSX-T without vSAN storage requires 9 ESXi hosts. The diagram below shows the topology for this deployment scenario. 

<img src="images/nsxt/pks-vs-nsxt-rpd.png" alt="RPD for PKS without vSAN">

The following subsections describe configuration details for the RPD of PKS without vSAN.

#### Management/Edge Cluster

The RPD for PKS without vSAN includes a Management/Edge Cluster with the following characteristics:

- Collapsed Management/Edge Cluster with 3 ESXi hosts.
- Each ESXi host runs 1 NSX-T Controller. The NSX-T Control Plane has 3 NSX-T Controllers total.
- 2 NSX-T Edge Nodes are deployed across 2 different ESXi hosts.

#### Compute Clusters

The RPD for PKS without vSAN includes 3 Compute Clusters with the following characteristic:

- Each Compute cluster has 2 ESXi hosts and is bound by a distinct availability zone (AZ) defined in BOSH Director.
	- Compute cluster1 (AZ1) with 2 ESXi hosts.
	- Compute cluster2 (AZ2) with 2 ESXi hosts.
	- Compute cluster3 (AZ3) with 2 ESXi hosts.
- Each Compute cluster runs 1 instance of a PKS-provisioned Kubernetes cluster with 3 master nodes per cluster and a per-plan number of worker nodes.

#### Storage (non-vSAN)

The RPD topology for PKS without vSAN requires the following storage configuration:

- All Compute Clusters are connected to same shared datastore that is used for persistent VM disks and Kubernetes Pod PV (Persistent Volumes). 
- All datastores can be collapses to single datastore, if needed.

#### Future Growth 

The RPD topology for PKS without vSAN can be scaled as follows to accommodate future growth requirements:

- The collapsed Management/Edge Cluster can expand up to 64 ESXi hosts.
- Each Compute Cluster can expand up to 64 ESXi hosts.

##<a id='pks-mpd'></a> MPD for PKS on vSphere with NSX-T

The minimum production deployment (MPD) topology represents the baseline requirements for running PKS on vSphere with NSX-T.

<p class="note"><strong>Note</strong>: The MPD topology is not recommended for production workloads.</p>

The diagram below shows the topology for this deployment scenario.

<img src="images/nsxt/pks-vs-nsxt-mpd.png" alt="MPD for PKS">

The following subsections describe configuration details for a MPD of PKS.

###<a id='pks-mpd-toplogy'></a> MPD Topology

<p class="note"><strong>Note:</strong> The MPD topology for PKS on vSphere with NSX-T applies to both vSAN and non-vSAN configurations.</p>

The MPD topology for PKS on vSphere with NSX-T requires the following minimum configuration: 

- A single collapsed Management/Edge/Compute cluster running 3 ESXi hosts in total. 
- Each ESXi host runs 1 NSX-T Controller. The NSX-T Control Plane has 3 NSX-T Controllers in total.
- Each ESXi host runs 1 Kubernetes master node. Each Kubernetes cluster has 3 master nodes in total.
- 2 NSX-T edge nodes deployed across 2 different ESXi hosts.
- The shared datastore (NFS/SCSI/â€¦) or vSAN datastore will be leveraged for VM disks and Kubernetes Pod PV (Persistent Volumes).
- The collapsed Management/Edge/Compute cluster can expand up to 64 ESXi hosts for future needs.

###<a id='pks-mpd-toplogy'></a> MPD Considerations

If you plan to deploy the MPD topology for PKS on vSphere with NSX-T, keep in mind the following considerations:

- When deploying an NSX-T Controller to each ESXi host, create a vSphere distributed resource scheduler (DRS) anti-affinity rule of type "separate virtual machines" for each of the 3 NSX-T Controllers.
- When deploying the NSX-T Edge Nodes across 2 different ESXi hosts, create a DRS anti-affinity rule of type "separate virtual machines" for both Edge Node VMs.
- Leverage vSphere resource pools to allocate proper hardware resources for the PKS Management Plane Components (tune reservation and limits accordingly).
- There is no fault tolerance for the Kubernetes cluster because PKS Availability Zones are not fully leveraged with this topology.
- The PKS AZ should be mapped to a vSphere Resource Pool.
- After deploying the Kubernetes cluster, you must manually make sure each master node is deployed to a different ESXi host (by tuning the DRS anti-affinity rule of type "separate virtual machines").
 
##<a id='vm-size-inventory'></a> VM Size Inventory

The following tables list the VMs and their sizes for PKS on vSphere with NSX-T deployments.

###<a id='control-plane-sizes'></a> Control Plane VM Sizes

The following table lists the resource requirements for each VM in the PKS infrastructure and control plane. 

VM 						| CPU   | Memory (GB) 	| Disk Space (GB)
------------------------|-------|---------------|----------------
vCenter (small size) 	| 4 	| 16 			| 290
NSX-T Manager			| 4		| 16			| 140
NSX-T Controller 1		| 4 	| 16 			| 120
NSX-T Controller 2		| 4 	| 16 			| 120
NSX-T Controller 3		| 4 	| 16 			| 120
Ops Manager 			| 1 	| 8 			| 160
BOSH Director 			| 2 	| 8 			| 103
Harbor Registry 		| 2 	| 8 			| 167
**TOTAL**				| **27**| **112**		| **1.25 TB**

###<a id='edge-node-sizes'></a> NSX-T Edge Nodes VM Sizes

VM 						| CPU   				| Memory (GB) 	| Disk Space (GB)
------------------------|-----------------------|---------------|----------------
NSX-T Edge Node 1		| 8 (Intel CPU only)	| 16 			| 120
NSX-T Edge Node 1		| 8 (Intel CPU only)	| 16 			| 120
**TOTAL**				| **16**				| **32**		| **.25 TB**

###<a id='k8s-node-sizes'></a> Kubernetes Cluster Nodes VM Sizes

The following table lists general sizing information:

VM 						| CPU   	| Memory (GB) 	| Ephemeral Disk Space | Persistent Disk Space
------------------------|-----------|---------------|----------------------|----------------------
Master Nodes 			| 1 to 16 	| 1 to 64 		| 8 to 256 GB 		   | 1 GB to 32 TB
Worker Nodes 			| 1 to 16 	| 1 to 64 		| 8 to 256 GB 		   | 1 GB to 32 TB


The following table shows sizing information for two example Kubernetes clusters with 3 master nodes and 5 worker nodes each.

VM 						| CPU per Node 	| Memory (GB) per Node 	| Ephemeral Disk Space per Node (GB) | Persistent Disk Space per Node (GB)
------------------------|---------------|-----------------------|------------------------------------|------------------------------------
Master Nodes (6 total)	| 2 			| 8 					| 64 		   				 		 | 128
Worker Nodes (10 total)	| 4 			| 16 					| 64 		   				 		 | 256

##<a id='hardware-reqs'></a> Hardware Requirements

The following tables list the VMs and their sizes for PKS on vSphere with NSX-T deployments.

###<a id='rpd-hardware'></a> RPD Hardware Requirements

The following table lists the hardware requirements for a RPD of PKS using vSAN.

VM 					| Number of Hosts 	| Total Cores per Host 		| Memory per Host (GB)  | NICs per Host | Shared Datastore
--------------------|-------------------|---------------------------|-----------------------|---------------|-----------------
Management Cluster	| 3 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Edge Cluster		| 2 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Compute Cluster	1	| 3 				| 6 (with Hyper Threading)	| 48 		   			| 2x 10GbE		| 192 GB
Compute Cluster	2	| 3 				| 6 (with Hyper Threading)	| 48 		   			| 2x 10GbE		| 192 GB
Compute Cluster	3	| 3 				| 6 (with Hyper Threading)	| 48 		   			| 2x 10GbE		| 192 GB

The following table lists the hardware requirements for a RPD of PKS without vSAN.

VM 					| Number of Hosts 	| Total Cores per Host 		| Memory per Host (GB)  | NICs per Host | Shared Datastore
--------------------|-------------------|---------------------------|-----------------------|---------------|-----------------
Management Cluster	| 3 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Edge Cluster		| 2 				| 16 (with Hyper Threading)	| 98 		   			| 2x 10GbE		| 1.5 TB
Compute Cluster	1	| 3 				| 10 (with Hyper Threading)	| 70 		   			| 2x 10GbE		| 192 GB
Compute Cluster	2	| 3 				| 10 (with Hyper Threading)	| 70 		   			| 2x 10GbE		| 192 GB
Compute Cluster	3	| 3 				| 10 (with Hyper Threading)	| 70 		   			| 2x 10GbE		| 192 GB

###<a id='mpd-hardware'></a> MPD Hardware Requirements

The following table shows the hardware requirements for a MPD of PKS where you have a collapsed Management, Edge, and Compute cluster.

VM 					| Number of Hosts 	| Total Cores per Host 		| Memory per Host (GB)  | NICs per Host | Shared Datastore
--------------------|-------------------|---------------------------|-----------------------|---------------|-----------------
Collapsed Cluster	| 3 				| 32 (with Hyper Threading)	| 236 		   			| 2x 10GbE		| 5.9 TB