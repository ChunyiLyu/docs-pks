---
title: Manage Users in UAA
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to manage users in Pivotal Container Service (PKS) with User Account and Authentication (UAA).
Create and manage users in UAA with the UAA Command Line Interface (UAAC).

## <a id='uaac'></a>How to Use UAAC

Use the UAA Command Line Interface (UAAC) to interact with the UAA server.
You can either run UAAC commands from the Ops Manager VM or install UAAC on your local workstation.

To run UAAC commands from the Ops Manager VM, see the following SSH procedures for [vSphere](#ssh-vsphere) or [Google Cloud Platform (GCP)](#ssh-gcp).

To install UAAC locally, see [Component: User Account and Authentication (UAA) Server](https://docs.pivotal.io/pivotalcf/concepts/architecture/uaa.html).

### <a id='ssh-vsphere'></a>SSH into the Ops Manager VM on vSphere

To SSH into the Ops Manager VM on vSphere, you need the credentials used to import the PCF .ova or .ovf file into your virtualization system.
You set these credentials when you installed Ops Manager.

<p class="note"><strong>Note</strong>: If you lose your credentials, you must shut down the Ops Manager VM in the vSphere UI and reset the password. See <a href="https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.security.doc/GUID-4BDBF79A-6C16-43B0-B0B1-637BF5516112.html">vCenter Password Requirements and Lockout Behavior</a> in the vSphere documentation for more information.</p>

1. From a command line, run `ssh ubuntu@OPS-MANAGER-FQDN` to SSH into the Ops Manager VM.
Replace `OPS-MANAGER-FQDN` with the fully qualified domain name of Ops Manager.

1. When prompted, enter the password that you set during the .ova deployment
into vCenter. For example:
  <pre class="terminal">
  $ ssh ubuntu&#64;my-opsmanager-fqdn.example.com
  Password: ***********
  </pre>

1. Proceed to the [Retrieve UAA Admin Credentials](#uaa-admin) section to manage users with UAAC.

### <a id='ssh-gcp'></a>SSH into the Ops Manager VM on GCP

To SSH into the Ops Manager VM in GCP, follow these instructions:

1. Confirm that you have installed the gcloud CLI. See the [Google Cloud Platform documentation](https://cloud.google.com/sdk/gcloud/#downloading_gcloud) for more information.

1. From the GCP console, click **Compute Engine**.

1. Locate the Ops Manager VM in the **VM Instances** list.

1. Click the **SSH** menu button.

1. Copy the SSH command that appears in the popup window.

1. Paste the command into your terminal window to SSH to the Ops Manager VM. For example:
    <pre class="terminal">
    $ gcloud compute ssh om-pcf-1a --zone us-central1-b
    </pre>

1. Run `sudo su - ubuntu` to switch to the `ubuntu` user.

1. Proceed to the [Retrieve UAA Admin Credentials](#uaa-admin) section to manage users with UAAC.

##<a id='uaa-admin-login'></a> Log in as an Admin

To retrieve the PKS UAA management admin client secret, do the following:

1. In a web browser, navigate to the fully qualified domain name (FQDN) of Ops Manager and click the **Pivotal Container Service** tile.
1. Click **Credentials**.
1. To view the secret, click **Link to Credential** next to **Pks Uaa Management Admin Client**. The client username is `admin`.
1. On the command line, run the following command to target your UAA server:
  <pre>`uaac target https://PKS-API:8443 --ca-cert ROOT-CA-FILENAME`</pre>
  Replace `PKS-API` with the URL to your PKS API server. You configured this URL in the PKS API section of _Installing PKS_ for your IaaS. For example, see [Installing PKS on vSphere](installing-pks-vsphere.html#pks-api). Replace `ROOT-CA-FILENAME` with the certificate file you downloaded in [Configure Access to the PKS API](configure-api.html#access).
  For example:
    <pre class="terminal">
    $ uaac target api.pks.example.com:8443 --ca-cert my-cert.cert
    </pre>

    <p class="note"><strong>Note</strong>: If you receive an <code>Unknown key: Max-Age = 86400</code> warning message, you can safely ignore it because it has no impact.</p>

1. Authenticate with UAA using the secret you retrieved in a previous step.
Run the following command, replacing `ADMIN-CLIENT-SECRET` with your PKS UAA management admin client secret: <pre>uaac token client get admin -s ADMIN-CLIENT-SECRET</pre>

##<a id='cluster-access'></a> Grant Cluster Access

You can assign the following UAA scopes to users, external LDAP groups, and clients:

  * `pks.clusters.manage`: accounts with this scope can create and access their own clusters.
  * `pks.clusters.admin`: accounts with this scope can create and access all clusters.

###<a id='uaa-user'></a> Grant Cluster Access to a User

To create a new UAA user with cluster access, perform the following steps:

1. Log in as the UAA admin using the procedure [above](#uaa-admin-login).

1. To create a new user, run the following command:
  <pre>uaac user add USERNAME --emails USER-EMAIL -p USER-PASSWORD</pre>

    For example:
    <pre class="terminal">$ uaac user add alana --emails alana&#64;example.com -p password</pre>

1. Assign a scope to the user to allow them to access Kubernetes clusters.
Run `uaac member add UAA-SCOPE USERNAME`, replacing `UAA-SCOPE` with one of the UAA scopes defined [above](#cluster-access).
For example:
    <pre class="terminal">$ uaac member add pks.clusters.admin alana</pre>

###<a id='external-group'></a> Grant Control Plane Access to an External LDAP Group

Connecting PKS to a LDAP external user store allows the User Account and Authentication (UAA) server to delegate authentication to existing enterprise user stores.

<p class='note'><strong>Note</strong>: When integrating with an external identity provider such as LDAP, authentication within the UAA becomes chained.
UAA first attempts to authenticate with a user's credentials against the UAA user store before the external provider, LDAP.
For more information, see <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-LDAP.md#chained-authentication">Chained Authentication</a> in the <em>User Account and Authentication LDAP Integration</em> GitHub documentation.
</p>

For more information about the process used by the UAA Server when it attempts to authenticate a user through LDAP, see the [Configuring LDAP Integration with Pivotal Cloud Foundry](https://discuss.zendesk.com/hc/en-us/articles/204140418-Configuring-LDAP-Integration-with-Pivotal-Cloud-Foundry-) Knowledge Base article.

The PKS control plane enables users to deploy and manage Kubernetes clusters.

To grant control plane access to an external LDAP group, perform the following steps:

1. Log in as the UAA admin using the procedure [above](#uaa-admin-login).

1. To assign the `pks.clusters.manage` scope to all users in an LDAP group, run the following command:
  <pre>uaac group map --name pks.clusters.manage GROUP-DISTINGUISHED-NAME</pre>
Replace `GROUP-DISTINGUISHED-NAME` with the LDAP Distinguished Name (DN) for the group.
For example:
    <pre class="terminal">
    $ uaac group map --name pks.clusters.manage Operators
    </pre>

1. (Optional) To assign the `pks.clusters.admin` scope to all users in an LDAP group, run the following command:
  <pre>uaac group map --name pks.clusters.admin GROUP-DISTINGUISHED-NAME</pre>
Replace `GROUP-DISTINGUISHED-NAME` with the LDAP DN for the group.
For example:
    <pre class="terminal">
    $ uaac group map --name pks.clusters.admin Operators
    </pre>

###<a id='uaa-client'></a> Grant Cluster Access to a Client

To grant cluster access to an automated client for a script or service, perform the following steps:

1. Log in as the UAA admin using the procedure [above](#uaa-admin-login).

1. Create a client with the desired scopes by running the following command:
  <pre>uaac client add CLIENT-NAME -s CLIENT-SECRET \
  --authorized&#95;grant&#95;types client&#95;credentials \
  --authorities UAA-SCOPES</pre>
  Replace `CLIENT-NAME` and `CLIENT-SECRET` with the client credentials. Replace `UAA-SCOPES` with one or more of the UAA scopes defined [above](#cluster-access), separated by a comma. For example:
    <pre class="terminal">$ uaac client add automated-client \
    -s randomly-generated-secret
    --authorized&#95;grant&#95;types client&#95;credentials  \
    --authorities pks.clusters.admin,pks.clusters.manage</pre>

###<a id='access-other-users'></a> Granting Cluster Access to Other Users

This process enables users to use kubectl to connect to clusters that have been created previously by admins. 

In this process, admins can grant cluster access to users. Users are given cluster access through cluster rolebinding. For more information on cluster role bindings, see [RoleBinding and ClusterRoleBinding
](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding) in the Kubernetes documentation. 

<p class='note'><strong>Note</strong>: To grant this kind of cluster access, ensure that you have selected **Enable UAA as OIDC provider** in the UAA section of the PKS tile. 
</p>

1. Log in as the UAA admin using the procedure [above](#uaa-admin-login).

1. Using the PKS CLI, run the `get-credentials` command to ensure that you can successfully access the cluster as an admin. As a result of running `get-credentials`, PKS automatically creates a cluster rolebinding for the UAA admin with the `pks.clusters.admin` and `pks.clusters.manage` scopes.
<!-- ^^ creates a cluster rolebinding for the admin but the point of this is for the admin to create a cluster rolebinding for the user -->
1. Create a spec YML file containing a clusterrole binding with the appropriate `roleRef` for the user. For additional information on the types of RoleRefs you can have in your spec file, consult RoleBinding and [ClusterRoleBinding](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding) in the Kubernetes documentation. 
For example:

<pre>
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: NAME-OF-CLUSTER-ROLEBINDING
subjects:
- kind: User
  name: NAME-OF-USER # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: ROLE-REF
  apiGroup: rbac.authorization.k8s.io
</pre>


1. Run the following command to create the above defined clusterrole binding in the cluster:
<pre class="terminal">kubectl apply -f spec.yml</pre>

1. The admin creates a partially completed kubeconfig, detailing: 
+ `clusters.cluster.certificate-authority-data`
+ `clusters.cluster.server`
+ `cluster.name`
+ `contexts.context.cluster`
+ `contexts.context.name`
+ `current-context`
+ `users.user.auth-provider.config.idp-issuer-url`

1. The following parameters are entered by the user:
+ `contexts.context.user`
+ `users.name`
+ `users.user.auth-provider.config.id-token`
+ `users.user.auth-provider.config.refresh-token`

1. To obtain  `users.user.auth-provider.config.id-token` and `users.user.auth-provider.config.refresh-token` run the following command:
<pre class="terminal">
curl 'https://<pks-domain-name>:8443/oauth/token' -k -XPOST -H 'Accept: application/json' -d "client_id=pks_cluster_client&client_secret=""&grant_type=password&username=UAA_USERNAME&password=UAA_PASSWORD&response_type=id_token"
</pre>
Where:
+ `UAA_USERNAME` is the user's UAA username.
+ `UAA_PASSWORD` is the user's UAA password. 
<!--- ^^ this is the user's uaa creds not the admins-->
1. From the output of the above command, take note of id_token and refresh_token values for the kubeconfig.
1. User saves kubeconfig to `$HOME/.kube/config` directory.


Review example kubeconfig file below:
<pre>
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: PROVIDED-BY-ADMIN
    server: PROVIDED-BY-ADMIN
  name: PROVIDED-BY-ADMIN
contexts:
- context:
    cluster: PROVIDED-BY-ADMIN
    user: PROVIDED-BY-USER
  name:  PROVIDED-BY-ADMIN
current-context: PROVIDED-BY-ADMIN
kind: Config
preferences: {}
users:
- name:  PROVIDED-BY-USER
  user: 
    auth-provider:
      config:
        client-id: pks_cluster_client
        cluster_client_secret: ""
        id-token: PROVIDED-BY-USER
        idp-issuer-url: < https://PROVIDED-BY-ADMIN:8443/oauth/token>
        refresh-token:  PROVIDED-BY-USER
      name: oidc
</pre>

For more information about organizing information using kubeconfig, see [Organizing Cluster Access Using kubeconfig Files](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) in the Kubernetes documentation.

Once the kubeconfig is constructed and saved to $HOME/.kube/config file, user should be able to connect to the cluster using `kubectl`. 
